\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite {Kri}.\relax }}{2}{figure.caption.6}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace {} y_{s_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace {} y_{s_m} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , s_m$, and fires $y_j$, sent to the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. The neuron threshold $\theta _j$ is reported within its body.\relax }}{4}{figure.caption.7}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace {} y_{s_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace {} y_{s_m}, \tmspace +\thinmuskip {.1667em} -\theta _j {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , s_m, \tmspace +\thinmuskip {.1667em} b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. Observe that, conversely to the model offered in Figure \ref {fig:neural-model}, the neuron threshold is now set to $0$.\relax }}{6}{figure.caption.8}
\contentsline {figure}{\numberline {1.4}{\ignorespaces Left: logistic function \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:logistic}\unskip \@@italiccorr )}} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resemble the Heaviside function. Right: hyperbolic tangent.\relax }}{7}{figure.caption.9}
\contentsline {figure}{\numberline {1.5}{\ignorespaces A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }}{10}{figure.caption.10}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Representation of the boundary conditions for the Laplace problems \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:laplace-problem-weight-function}\unskip \@@italiccorr )}} (left) and \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:laplace-problem-projection-function}\unskip \@@italiccorr )}} (right) stated on a exagonal reference domain $\Omega $.\relax }}{36}{figure.caption.15}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }}{37}{figure.caption.16}
\contentsline {figure}{\numberline {2.3}{\ignorespaces The parametrized computational domain (solid line) for the Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:nonlinear-poisson-example}\unskip \@@italiccorr )}}.\relax }}{58}{figure.caption.17}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: average relative error between the FE solution for the problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:nonlinear-poisson-example}\unskip \@@italiccorr )}} and either its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal {P}$ consisting of $N_{te} = 50$ randomly picked values. Right: comparison between the online run times for the FE (full-order) scheme and the POD-Galerkin (reduced-order) method on $\Xi _{te}$.\relax }}{59}{figure.caption.18}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Three point sets randomly generate through the latin hypercube sampling. Observe the good coverage provided by the union of the sets, with only a few overlapping points.\relax }}{61}{figure.caption.19}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: error analysis for the POD-G RB method applied to Equation \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-1}\unskip \@@italiccorr )}}; the results refer to a POD basis constructed based on $N = 25$, $50$, $100$ snapshots; for $N = 100$, the singular values of the correspondent snapshot matrix are shown as well. Right: FE and POD-G solutions for three parameter values.\relax }}{69}{figure.caption.20}
\contentsline {figure}{\numberline {3.2}{\ignorespaces In respect to the approximation of the map \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:map-to-approximate}\unskip \@@italiccorr )}} for the Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-1}\unskip \@@italiccorr )}} via neural networks, comparison between three supervised algorithms - Levenberg-Marquardt (blue), Scaled Conjugate Gradient (orange), BFGS Quasi-Newton (yellow) - in terms of the optimal configuration detected (\emph {right}) and the associated test error (\emph {right}) for different amounts of training samples.\relax }}{70}{figure.caption.21}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Left: relative error yielded by the POD-NN RB method applied to Equation \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-1}\unskip \@@italiccorr )}}; the convergence to the projection error for $L = 10$ (solid line) is analyzed in terms of both the number of neurons included in the neural network and the dimension of the training set. Right: FE and POD-NN solutions for three parameter values.\relax }}{70}{figure.caption.21}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Error analysis for the POD-Galerkin RB method applied to the problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-2}\unskip \@@italiccorr )}}. The lower-bound provided by the projection error \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:projection-error}\unskip \@@italiccorr )}} is reported as reference.\relax }}{71}{figure.caption.22}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Left: error analysis for the POD-NN RB method (dotted lines) applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-2}\unskip \@@italiccorr )}}. The solid solid sections refer to the steps carried out by the routine \ref {alg:podnn-training}; the error yielded by the POD-G method using $L = 8$ or $L = 15$ basis functions are reported as reference. Right: comparison between FE and POD-NN solutions for three input vectors.\relax }}{72}{figure.caption.23}
\contentsline {figure}{\numberline {3.6}{\ignorespaces From left to right, top to bottom: regression plots for the first, seventh, eighth and nine scalar components of the outputs provided on $\Xi _{te}$ by a network with $H_1 = H_2 = 25$ neurons per hidden layer, trained to approximate the map \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:map-to-approximate}\unskip \@@italiccorr )}} for problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-2}\unskip \@@italiccorr )}}.\relax }}{73}{figure.caption.24}
\contentsline {figure}{\numberline {3.7}{\ignorespaces The first $10$ POD basis functions for the (nonlinear) Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-3}\unskip \@@italiccorr )}}.\relax }}{74}{figure.caption.25}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\psi _1$}}}{74}{subfigure.7.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\psi _2$}}}{74}{subfigure.7.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {$\psi _3$}}}{74}{subfigure.7.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {$\psi _4$}}}{74}{subfigure.7.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {$\psi _5$}}}{74}{subfigure.7.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {$\psi _6$}}}{74}{subfigure.7.6}
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {$\psi _7$}}}{74}{subfigure.7.7}
\contentsline {subfigure}{\numberline {(h)}{\ignorespaces {$\psi _8$}}}{74}{subfigure.7.8}
\contentsline {subfigure}{\numberline {(i)}{\ignorespaces {$\psi _9$}}}{74}{subfigure.7.9}
\contentsline {subfigure}{\numberline {(j)}{\ignorespaces {$\psi _{10}$}}}{74}{subfigure.7.10}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Convergence analysis for the POD-G and POD-NN methods applied to the problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-3}\unskip \@@italiccorr )}} (\emph {left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph {right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer, and considering $L = 10$ POD modes.\relax }}{74}{figure.caption.26}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Sensitivity analysis for the POD-NN method applied to the problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-3}\unskip \@@italiccorr )}} with respect to the number of POD coefficients (per sample) used during the training; to prevent overfitting, $N_{tr} = 300$ learning patterns have been used.\relax }}{75}{figure.caption.27}
\contentsline {figure}{\numberline {3.10}{\ignorespaces The quadrilateral domain used in the simulations (\emph {left}, solid line) and the parametrizations of its sides (\emph {right}).\relax }}{76}{figure.caption.28}
\contentsline {figure}{\numberline {3.11}{\ignorespaces The stenosis geometry employed in the simulations (\emph {left}) and the parametrizations of its sides (\emph {right}).\relax }}{76}{figure.caption.28}
\contentsline {figure}{\numberline {3.12}{\ignorespaces Online run times for the FE, POD-G and POD-NN methods applied to the problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-1}\unskip \@@italiccorr )}}, for $N_{te} = 100$ randomly generated parameter values. \relax }}{78}{figure.caption.29}
\contentsline {figure}{\numberline {3.13}{\ignorespaces On the right, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods applied to the problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-1}\unskip \@@italiccorr )}}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the left. The solid tracts denote the steps actually pursued by the automatic training routine \ref {alg:podnn-training}. All the results refer to neural networks exposed to $L = 15$ POD coefficients per learning sample.\relax }}{78}{figure.caption.30}
\contentsline {figure}{\numberline {3.14}{\ignorespaces Finite element (\emph {left}), POD-G (\emph {center}) and POD-NN (\emph {right}) solution to the semilinear Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-2}\unskip \@@italiccorr )}} re-stated over the reference domain, with $\bm {\mu } = (0.835, \tmspace +\thinmuskip {.1667em} 0.034)$.\relax }}{79}{figure.caption.31}
\contentsline {figure}{\numberline {3.15}{\ignorespaces Online relative errors (\emph {left}) and run times (\emph {right}) for the POD-G and the POD-NN methods applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-2}\unskip \@@italiccorr )}} for $N_{te} = 50$ randomly picked parameter values.\relax }}{80}{figure.caption.32}
\contentsline {figure}{\numberline {3.16}{\ignorespaces Convergence analysis for the POD-NN RB method applied to the BVP \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-2}\unskip \@@italiccorr )}}. The results have been obtained via three-layers (i.e., two hidden layers plus the output layer) neural networks with $H_1 = H_2 \in {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }15, \tmspace +\thinmuskip {.1667em} 20, \tmspace +\thinmuskip {.1667em} 25, \tmspace +\thinmuskip {.1667em} 30, \tmspace +\thinmuskip {.1667em} 35 {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ neurons in each hidden layer, and trained with $N_{tr} \in {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }100, \tmspace +\thinmuskip {.1667em} 200, \tmspace +\thinmuskip {.1667em} 300 {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ learning patterns. Each pattern consists of an input vector $\bm {\mu } \in \mathcal {P}$ and the first $L = 35$ POD coefficients ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } \mathbf {u}_h(\bm {\mu }), \bm {\psi }_i {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }_{\math@bb {R}^M}$, $i = 1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , L$, as teaching inputs.\relax }}{80}{figure.caption.33}
\contentsline {figure}{\numberline {3.17}{\ignorespaces On the left, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods (\emph {left}) applied to the problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-3}\unskip \@@italiccorr )}}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the right. The solid tracts denote the steps actually pursued by the automatic training routine \ref {alg:podnn-training}. All the results refer to neural networks exposed to $L = 30$ POD coefficients per sample during the learning step.\relax }}{81}{figure.caption.34}
\contentsline {figure}{\numberline {3.18}{\ignorespaces \relax }}{82}{figure.caption.35}
