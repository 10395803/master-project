\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite {Kri}.\relax }}{8}{figure.caption.4}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace {} y_{s_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace {} y_{s_m} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , s_m$, and fires $y_j$, sent to the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. The neuron threshold $\theta _j$ is reported within its body.\relax }}{10}{figure.caption.5}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace {} y_{s_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace {} y_{s_m}, \tmspace +\thinmuskip {.1667em} -\theta _j {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , s_m, \tmspace +\thinmuskip {.1667em} b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. Observe that, conversely to the model offered in Figure \ref {fig:neural-model}, the neuron threshold is now set to $0$.\relax }}{12}{figure.caption.6}
\contentsline {figure}{\numberline {1.4}{\ignorespaces Left: logistic function \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:logistic}\unskip \@@italiccorr )}} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resemble the Heaviside function. Right: hyperbolic tangent.\relax }}{13}{figure.caption.7}
\contentsline {figure}{\numberline {1.5}{\ignorespaces A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }}{16}{figure.caption.8}
\addvspace {10\p@ }
