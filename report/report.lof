\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite {Kri}.\relax }}{2}{figure.caption.6}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace {} y_{s_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace {} y_{s_m} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , s_m$, and fires $y_j$, sent to the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. The neuron threshold $\theta _j$ is reported within its body.\relax }}{4}{figure.caption.7}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace {} y_{s_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace {} y_{s_m}, \tmspace +\thinmuskip {.1667em} -\theta _j {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , s_m, \tmspace +\thinmuskip {.1667em} b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. Observe that, conversely to the model offered in Figure \ref {fig:neural-model}, the neuron threshold is now set to $0$.\relax }}{6}{figure.caption.8}
\contentsline {figure}{\numberline {1.4}{\ignorespaces Left: logistic function \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:logistic}\unskip \@@italiccorr )}} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resemble the Heaviside function. Right: hyperbolic tangent.\relax }}{7}{figure.caption.9}
\contentsline {figure}{\numberline {1.5}{\ignorespaces A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }}{10}{figure.caption.10}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Representation of the boundary conditions for the Laplace problems \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:laplace-problem-weight-function}\unskip \@@italiccorr )}} (left) and \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:laplace-problem-projection-function}\unskip \@@italiccorr )}} (right) stated on a exagonal reference domain $\Omega $.\relax }}{36}{figure.caption.15}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }}{37}{figure.caption.16}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Left: computational domain $\mathaccentV {widetilde}365{\Omega } = \mathaccentV {widetilde}365{\Omega }(\bm {\mu })$ (solid line) for the linear Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:linear-poisson}\unskip \@@italiccorr )}}. Right: average relative error committed by approximating the FE solution either through its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal {P}$ consisting of $N_{te} = 200$ randomly picked values.\relax }}{58}{figure.caption.17}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: computational domain $\mathaccentV {widetilde}365{\Omega } = \mathaccentV {widetilde}365{\Omega }(\bm {\mu })$ (solid line) for the nonlinear Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:linear-poisson}\unskip \@@italiccorr )}}, with $H = 2$. Right: average relative error committed by approximating the FE solution either through its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue). The errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal {P}$ consisting of $N_{te} = 200$ randomly picked values.\relax }}{59}{figure.caption.18}
\addvspace {10\p@ }
