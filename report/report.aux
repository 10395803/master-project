\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\citation{Kri}
\citation{Hag14}
\citation{Hay05}
\citation{Kri}
\citation{Kri}
\citation{Kri}
\citation{Hay05}
\citation{Kri}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to neural networks}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Introduction to neural networks}{{1}{1}{Introduction to neural networks}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Biological motivation}{1}{section.1.1}}
\newlabel{section:Biological motivation}{{1.1}{1}{Biological motivation}{section.1.1}{}}
\citation{SD13}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite  {Kri}.\relax }}{2}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neuron}{{1.1}{2}{Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite {Kri}.\relax }{figure.caption.6}{}}
\citation{Hag14}
\citation{Kri}
\citation{Kri}
\citation{Nie15}
\citation{SD13}
\citation{Kri}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Artificial neural networks}{3}{section.1.2}}
\newlabel{section:Artificial neural networks}{{1.2}{3}{Artificial neural networks}{section.1.2}{}}
\newlabel{def:neural-network}{{1.1}{3}{Neural network}{definition.1.1}{}}
\citation{Kri}
\citation{Kri}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace  {} y_{s_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace  {} y_{s_m} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , s_m$, and fires $y_j$, sent to the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. The neuron threshold $\theta _j$ is reported within its body.\relax }}{4}{figure.caption.7}}
\newlabel{fig:neural-model}{{1.2}{4}{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big \lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m} \big \rbrace $ coming from the sending neurons $s_1, \, \ldots \, , s_m$, and fires $y_j$, sent to the target neurons $\big \lbrace r_1, \, \ldots \, , r_n \big \rbrace $ through the synapsis $\big \lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big \rbrace $. The neuron threshold $\theta _j$ is reported within its body.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Neuronal model}{4}{subsection.1.2.1}}
\newlabel{section:Neuronal model}{{1.2.1}{4}{Neuronal model}{subsection.1.2.1}{}}
\newlabel{eq:propagation-function}{{1.1}{4}{Neuronal model}{equation.1.1}{}}
\newlabel{eq:weighted-sum}{{1.2}{4}{Neuronal model}{equation.1.2}{}}
\newlabel{eq:activation-function}{{1.3}{4}{Neuronal model}{equation.1.3}{}}
\citation{Hay05}
\citation{Hay05}
\newlabel{eq:net-input}{{1.4}{5}{Neuronal model}{equation.1.4}{}}
\newlabel{eq:heaviside}{{1.6}{5}{Neuronal model}{equation.1.6}{}}
\newlabel{eq:logistic}{{1.7}{5}{Neuronal model}{equation.1.7}{}}
\citation{Hay05}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace  {} y_{s_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace  {} y_{s_m}, \tmspace  +\thinmuskip {.1667em} -\theta _j {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , s_m, \tmspace  +\thinmuskip {.1667em} b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. Observe that, conversely to the model offered in Figure \ref  {fig:neural-model}, the neuron threshold is now set to $0$.\relax }}{6}{figure.caption.8}}
\newlabel{fig:neural-model-bias}{{1.3}{6}{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big \lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m}, \, -\theta _j \big \rbrace $ coming from the sending neurons $s_1, \, \ldots \, , s_m, \, b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons $\big \lbrace r_1, \, \ldots \, , r_n \big \rbrace $ through the synapsis $\big \lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big \rbrace $. Observe that, conversely to the model offered in Figure \ref {fig:neural-model}, the neuron threshold is now set to $0$.\relax }{figure.caption.8}{}}
\newlabel{eq:hyperbolic-tangent}{{1.8}{6}{Neuronal model}{equation.1.8}{}}
\newlabel{eq:output-function}{{1.9}{6}{Neuronal model}{equation.1.9}{}}
\citation{Ros58}
\citation{Kri}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Left: logistic function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logistic}\unskip \@@italiccorr )}} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resemble the Heaviside function. Right: hyperbolic tangent.\relax }}{7}{figure.caption.9}}
\newlabel{fig:activation-functions}{{1.4}{7}{Left: logistic function \eqref {eq:logistic} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resemble the Heaviside function. Right: hyperbolic tangent.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Network topologies: the feedforward neural network}{7}{subsection.1.2.2}}
\newlabel{section:Network topologies}{{1.2.2}{7}{Network topologies: the feedforward neural network}{subsection.1.2.2}{}}
\citation{Cyb88}
\citation{Cyb89}
\citation{Cyb89}
\citation{Cyb88}
\citation{Hop82}
\citation{Koh98}
\newlabel{cybenko-first-rule}{{{{(i)}}}{8}{Network topologies: the feedforward neural network}{Item.1}{}}
\newlabel{cybenko-second-rule}{{{{(ii)}}}{8}{Network topologies: the feedforward neural network}{Item.2}{}}
\citation{Hay05}
\citation{Kri}
\citation{Kri}
\citation{KLM96}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Training a multilayer feedforward neural network}{9}{subsection.1.2.3}}
\newlabel{section:Training a multilayer feedforard neural network}{{1.2.3}{9}{Training a multilayer feedforward neural network}{subsection.1.2.3}{}}
\citation{Heb49}
\citation{Hay05}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }}{10}{figure.caption.10}}
\newlabel{fig:neural-network}{{1.5}{10}{A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }{figure.caption.10}{}}
\newlabel{a}{{{{(a)}}}{10}{Training a multilayer feedforward neural network}{Item.3}{}}
\newlabel{b}{{{{(b)}}}{10}{Training a multilayer feedforward neural network}{Item.4}{}}
\citation{Hay05}
\citation{Kri}
\newlabel{first-rule}{{{{(i)}}}{11}{Training a multilayer feedforward neural network}{Item.5}{}}
\newlabel{second-rule}{{{{(ii)}}}{11}{Training a multilayer feedforward neural network}{Item.6}{}}
\newlabel{first-rule}{{1.2.3}{11}{Training a multilayer feedforward neural network}{Item.6}{}}
\newlabel{eq:hebbian-rule}{{1.10}{11}{Training a multilayer feedforward neural network}{equation.1.10}{}}
\newlabel{eq:weight-update}{{1.11}{11}{Training a multilayer feedforward neural network}{equation.1.11}{}}
\newlabel{eq:generalized-hebbian-rule}{{1.12}{11}{Training a multilayer feedforward neural network}{equation.1.12}{}}
\newlabel{eq:performance-function}{{1.13}{11}{Training a multilayer feedforward neural network}{equation.1.13}{}}
\citation{Hay05}
\citation{MR86}
\newlabel{eq:accumulated-error}{{1.14}{12}{Training a multilayer feedforward neural network}{equation.1.14}{}}
\newlabel{eq:mse}{{1.15}{12}{Training a multilayer feedforward neural network}{equation.1.15}{}}
\newlabel{eq:accumulated-mse}{{1.16}{12}{Training a multilayer feedforward neural network}{equation.1.16}{}}
\newlabel{eq:antigradient}{{1.17}{12}{Training a multilayer feedforward neural network}{equation.1.17}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.1}{\ignorespaces Backbone of any supervised online learning algorithm; note that the full procedure ends when all training patterns yield an error which is below a defined threshold.\relax }}{13}{algorithm.1.1}}
\newlabel{alg:online-learning}{{1.1}{13}{Backbone of any supervised online learning algorithm; note that the full procedure ends when all training patterns yield an error which is below a defined threshold.\relax }{algorithm.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation of error}{13}{section*.11}}
\newlabel{section:Backpropagation of error}{{1.2.3}{13}{Backpropagation of error}{section*.11}{}}
\newlabel{eq:bp-first-equation}{{1.18}{13}{Backpropagation of error}{equation.1.18}{}}
\newlabel{eq:bp-second-equation}{{1.19}{13}{Backpropagation of error}{equation.1.19}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.2}{\ignorespaces Backbone of any supervised offline learning algorithm; the procedure to compute the accumulated error is provided as well.\relax }}{14}{algorithm.1.2}}
\newlabel{alg:offline-learning}{{1.2}{14}{Backbone of any supervised offline learning algorithm; the procedure to compute the accumulated error is provided as well.\relax }{algorithm.1.2}{}}
\newlabel{eq:bp-third-equation}{{1.20}{14}{Backpropagation of error}{equation.1.20}{}}
\newlabel{eq:delta}{{1.21}{14}{Backpropagation of error}{equation.1.21}{}}
\newlabel{eq:bp-weight-update}{{1.22}{14}{Backpropagation of error}{equation.1.22}{}}
\citation{Kri}
\newlabel{eq:bp-fourth-equation}{{1.23}{15}{Backpropagation of error}{equation.1.23}{}}
\newlabel{eq:bp-fifth-equation}{{1.24}{15}{Backpropagation of error}{equation.1.24}{}}
\newlabel{eq:bp-sixth-equation}{{1.25}{15}{Backpropagation of error}{equation.1.25}{}}
\citation{Kri}
\citation{WH60}
\citation{Kri}
\citation{Kri}
\citation{RB93}
\citation{Fah88}
\citation{Mar63}
\citation{Hag94}
\newlabel{eq:bp-inner-neuron}{{1.26a}{16}{Backpropagation of error}{equation.1.26}{}}
\newlabel{eq:bp-output-neuron}{{1.26b}{16}{Backpropagation of error}{equation.1.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{Levenberg-Marquardt algorithm}{16}{section*.12}}
\newlabel{section:Levenberg-Marquardt algorithm}{{1.2.3}{16}{Levenberg-Marquardt algorithm}{section*.12}{}}
\citation{Hag94}
\citation{Mar63}
\citation{Mar63}
\newlabel{eq:newton}{{1.27}{17}{Levenberg-Marquardt algorithm}{equation.1.27}{}}
\newlabel{eq:accumulated-mse-bis}{{1.28}{17}{Levenberg-Marquardt algorithm}{equation.1.28}{}}
\newlabel{eq:jacobian}{{1.29}{17}{Levenberg-Marquardt algorithm}{equation.1.29}{}}
\newlabel{eq:gradient}{{1.30}{17}{Levenberg-Marquardt algorithm}{equation.1.30}{}}
\newlabel{eq:hessian}{{1.31}{17}{Levenberg-Marquardt algorithm}{equation.1.31}{}}
\newlabel{eq:newton-quadratic-function}{{1.32}{17}{Levenberg-Marquardt algorithm}{equation.1.32}{}}
\newlabel{eq:levenberg-marquardt}{{1.33}{17}{Levenberg-Marquardt algorithm}{equation.1.33}{}}
\newlabel{eq:jacobian-entry}{{1.34}{18}{Levenberg-Marquardt algorithm}{equation.1.34}{}}
\newlabel{eq:jacobian-entry-equation}{{1.35}{18}{Levenberg-Marquardt algorithm}{equation.1.35}{}}
\newlabel{eq:levenberg-marquardt-delta}{{1.36}{18}{Levenberg-Marquardt algorithm}{equation.1.36}{}}
\newlabel{eq:jacobian-first-case}{{1.37}{18}{Levenberg-Marquardt algorithm}{equation.1.37}{}}
\newlabel{eq:jacobian-second-case}{{1.38}{18}{Levenberg-Marquardt algorithm}{equation.1.38}{}}
\citation{Mar63}
\citation{Hag94}
\newlabel{eq:jacobian-third-case}{{1.39}{19}{Levenberg-Marquardt algorithm}{equation.1.39}{}}
\newlabel{eq:levenberg-marquardt-inner-neuron}{{1.40a}{19}{Levenberg-Marquardt algorithm}{equation.1.40}{}}
\newlabel{eq:levenberg-marquardt-output-neuron}{{1.40b}{19}{Levenberg-Marquardt algorithm}{equation.1.40}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.3}{\ignorespaces An iteration of the Levenberg-Marquardt training algorithm.\relax }}{19}{algorithm.1.3}}
\newlabel{alg:levenberg-marquardt}{{1.3}{19}{An iteration of the Levenberg-Marquardt training algorithm.\relax }{algorithm.1.3}{}}
\citation{Kri}
\citation{Mat16}
\citation{Koh95}
\citation{Mat16}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Practical considerations on the design of artificial neural networks}{20}{subsection.1.2.4}}
\newlabel{section:Practical considerations on the design of artificial neural networks}{{1.2.4}{20}{Practical considerations on the design of artificial neural networks}{subsection.1.2.4}{}}
\citation{OBS}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.4}{\ignorespaces The complete training algorithm adopted in our numerical tests.\relax }}{22}{algorithm.1.4}}
\newlabel{alg:offline-learning-complete}{{1.4}{22}{The complete training algorithm adopted in our numerical tests.\relax }{algorithm.1.4}{}}
\citation{MN16}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reduced basis methods for nonlinear partial differential equations}{23}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Reduced Basis method for nonlinear partial differential equations}{{2}{23}{Reduced basis methods for nonlinear partial differential equations}{chapter.2}{}}
\newlabel{eq:geometric-map}{{2}{23}{Reduced basis methods for nonlinear partial differential equations}{chapter.2}{}}
\citation{HSR16}
\citation{JIR14}
\citation{QMN15}
\newlabel{eq:map-continuous}{{2.1}{24}{Reduced basis methods for nonlinear partial differential equations}{equation.2.1}{}}
\newlabel{eq:nonlinear-system-full}{{2.2}{24}{Reduced basis methods for nonlinear partial differential equations}{equation.2.2}{}}
\newlabel{eq:reduced-solution-algebraic}{{2.3}{24}{Reduced basis methods for nonlinear partial differential equations}{equation.2.3}{}}
\newlabel{eq:reduced-solution}{{2.4}{24}{Reduced basis methods for nonlinear partial differential equations}{equation.2.4}{}}
\citation{MN16}
\citation{Pru02}
\citation{HSR16}
\citation{QMN15}
\citation{MN16}
\citation{Bar04}
\citation{NMA15}
\citation{HSR16}
\citation{QMN15}
\citation{Bal14}
\newlabel{eq:nonlinear-system-reduced}{{2.5}{25}{Reduced basis methods for nonlinear partial differential equations}{equation.2.5}{}}
\citation{Ams10}
\citation{Chen17}
\citation{Haa13}
\newlabel{eq:interpolation-function}{{2.8}{26}{Reduced basis methods for nonlinear partial differential equations}{equation.2.8}{}}
\citation{Qua10}
\citation{MM10}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Parametrized nonlinear PDEs}{27}{section.2.1}}
\newlabel{section:Parametrized nonlinear PDEs}{{2.1}{27}{Parametrized nonlinear PDEs}{section.2.1}{}}
\newlabel{eq:pde-differential-form}{{2.10}{27}{Parametrized nonlinear PDEs}{equation.2.10}{}}
\newlabel{eq:pde-variational-form}{{2.11}{27}{Parametrized nonlinear PDEs}{equation.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Nonlinear Poisson equation}{27}{subsection.2.1.1}}
\newlabel{section:Nonlinear Poisson equation}{{2.1.1}{27}{Nonlinear Poisson equation}{subsection.2.1.1}{}}
\newlabel{eq:poisson-differential}{{2.12}{27}{Nonlinear Poisson equation}{equation.2.12}{}}
\newlabel{eq:poisson-differential-first-equation}{{2.12a}{27}{Nonlinear Poisson equation}{equation.2.12a}{}}
\citation{Ran99}
\newlabel{eq:poisson-weak-derivation}{{2.13}{28}{Nonlinear Poisson equation}{equation.2.13}{}}
\newlabel{eq:poisson-weak-forms}{{2.14}{28}{Nonlinear Poisson equation}{equation.2.14}{}}
\newlabel{eq:poisson-weak}{{2.15}{28}{Nonlinear Poisson equation}{equation.2.15}{}}
\citation{QMN15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Steady Navier-Stokes equations}{29}{subsection.2.1.2}}
\newlabel{section:Steady Navier-Stokes equations}{{2.1.2}{29}{Steady Navier-Stokes equations}{subsection.2.1.2}{}}
\newlabel{eq:ns-differential}{{2.16}{29}{Steady Navier-Stokes equations}{equation.2.16}{}}
\newlabel{eq:mass-conservation}{{2.16a}{29}{Steady Navier-Stokes equations}{equation.2.16a}{}}
\newlabel{eq:momentum-conservation}{{2.16b}{29}{Steady Navier-Stokes equations}{equation.2.16b}{}}
\newlabel{eq:ns-weak}{{2.17}{29}{Steady Navier-Stokes equations}{equation.2.17}{}}
\newlabel{eq:ns-weak-velocity}{{2.17a}{29}{Steady Navier-Stokes equations}{equation.2.17a}{}}
\newlabel{eq:ns-weak-pressure}{{2.17b}{29}{Steady Navier-Stokes equations}{equation.2.17b}{}}
\newlabel{eq:ns-weak-forms}{{2.18}{29}{Steady Navier-Stokes equations}{equation.2.18}{}}
\newlabel{eq:ns-weak-forms-c}{{2.18a}{29}{Steady Navier-Stokes equations}{equation.2.18a}{}}
\newlabel{eq:ns-weak-forms-a}{{2.18b}{29}{Steady Navier-Stokes equations}{equation.2.18b}{}}
\newlabel{eq:ns-weak-forms-b}{{2.18c}{30}{Steady Navier-Stokes equations}{equation.2.18c}{}}
\newlabel{eq:ns-weak-forms-d}{{2.18d}{30}{Steady Navier-Stokes equations}{equation.2.18d}{}}
\newlabel{eq:ns-weak-forms-f1}{{2.18e}{30}{Steady Navier-Stokes equations}{equation.2.18e}{}}
\newlabel{eq:ns-weak-forms-f2}{{2.18f}{30}{Steady Navier-Stokes equations}{equation.2.18f}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}From the original to the reference domain}{30}{section.2.2}}
\newlabel{section:From the original to the reference domain}{{2.2}{30}{From the original to the reference domain}{section.2.2}{}}
\citation{JIR14}
\newlabel{first-compatibility-condition}{{{{(a)}}}{31}{From the original to the reference domain}{Item.12}{}}
\newlabel{second-compatibility-condition}{{{{(b)}}}{31}{From the original to the reference domain}{Item.13}{}}
\newlabel{eq:parametrized-map}{{2.19}{31}{From the original to the reference domain}{equation.2.19}{}}
\newlabel{eq:pde-differential-reference}{{2.20}{31}{From the original to the reference domain}{equation.2.20}{}}
\newlabel{eq:pde-weak-reference}{{2.21}{31}{From the original to the reference domain}{equation.2.21}{}}
\newlabel{eq:parametrized-map-discrete}{{2.23}{31}{From the original to the reference domain}{equation.2.23}{}}
\citation{Rud64}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Change of variables formulae}{32}{subsection.2.2.1}}
\newlabel{section:Change of variables formulae}{{2.2.1}{32}{Change of variables formulae}{subsection.2.2.1}{}}
\newlabel{eq:chain-rule-component}{{2.24}{32}{Change of variables formulae}{equation.2.24}{}}
\newlabel{eq:chain-rule}{{2.25}{32}{Change of variables formulae}{equation.2.25}{}}
\citation{Rud64}
\newlabel{eq:chain-rule-bis}{{2.26}{33}{Change of variables formulae}{equation.2.26}{}}
\newlabel{eq:change-of-variables}{{2.27}{33}{Change of variables formulae}{equation.2.27}{}}
\newlabel{eq:change-of-variables-vectorial}{{2.28}{33}{Change of variables formulae}{equation.2.28}{}}
\newlabel{eq:change-of-variables-first}{{2.29}{33}{Change of variables formulae}{equation.2.29}{}}
\newlabel{eq:change-of-variables-second}{{2.30}{33}{Change of variables formulae}{equation.2.30}{}}
\newlabel{eq:change-of-variables-third}{{2.31}{33}{Change of variables formulae}{equation.2.31}{}}
\newlabel{eq:change-of-variables-fourth}{{2.32}{33}{Change of variables formulae}{equation.2.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The problems of interest}{34}{subsection.2.2.2}}
\newlabel{section:The problems of interest}{{2.2.2}{34}{The problems of interest}{subsection.2.2.2}{}}
\newlabel{eq:poisson-weak-reference}{{2.33}{34}{The problems of interest}{equation.2.33}{}}
\newlabel{eq:poisson-weak-forms-reference}{{2.34}{34}{The problems of interest}{equation.2.34}{}}
\newlabel{eq:poisson-weak-forms-reference-first}{{2.34a}{34}{The problems of interest}{equation.2.34a}{}}
\newlabel{eq:poisson-weak-forms-reference-second}{{2.34b}{34}{The problems of interest}{equation.2.34b}{}}
\newlabel{eq:ns-weak-reference}{{2.35}{34}{The problems of interest}{equation.2.35}{}}
\newlabel{eq:ns-weak-forms}{{2.36}{34}{The problems of interest}{equation.2.36}{}}
\newlabel{eq:ns-weak-forms-c-reference}{{2.36a}{34}{The problems of interest}{equation.2.36a}{}}
\newlabel{eq:ns-weak-forms-a-reference}{{2.36b}{34}{The problems of interest}{equation.2.36b}{}}
\newlabel{eq:ns-weak-forms-b-reference}{{2.36c}{34}{The problems of interest}{equation.2.36c}{}}
\newlabel{eq:ns-weak-forms-d-reference}{{2.36d}{34}{The problems of interest}{equation.2.36d}{}}
\citation{JIR14}
\citation{JIR14}
\newlabel{eq:ns-weak-forms-f1-reference}{{2.36e}{35}{The problems of interest}{equation.2.36e}{}}
\newlabel{eq:ns-weak-forms-f2-reference}{{2.36f}{35}{The problems of interest}{equation.2.36f}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}The boundary displacement-dependent transfinite map (BDD TM)}{35}{subsection.2.2.3}}
\newlabel{section:The boundary displacement-dependent transfinite map}{{2.2.3}{35}{The boundary displacement-dependent transfinite map (BDD TM)}{subsection.2.2.3}{}}
\citation{JIR14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Representation of the boundary conditions for the Laplace problems \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:laplace-problem-weight-function}\unskip \@@italiccorr )}} (left) and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:laplace-problem-projection-function}\unskip \@@italiccorr )}} (right) stated on a exagonal reference domain $\Omega $.\relax }}{36}{figure.caption.15}}
\newlabel{fig:laplace-bc}{{2.1}{36}{Representation of the boundary conditions for the Laplace problems \eqref {eq:laplace-problem-weight-function} (left) and \eqref {eq:laplace-problem-projection-function} (right) stated on a exagonal reference domain $\Omega $.\relax }{figure.caption.15}{}}
\newlabel{eq:laplace-problem-weight-function}{{2.38}{36}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.38}{}}
\newlabel{eq:laplace-problem-projection-function}{{2.39}{36}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.39}{}}
\newlabel{eq:weight-projection-function}{{2.40}{36}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.40}{}}
\citation{Rud64}
\citation{JIR14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }}{37}{figure.caption.16}}
\newlabel{fig:bc-square}{{2.2}{37}{Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }{figure.caption.16}{}}
\newlabel{eq:bddtm}{{2.41}{37}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.41}{}}
\newlabel{eq:my-bddtm}{{2.42}{37}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Well-posedness of the test cases}{37}{section.2.3}}
\newlabel{section:Well-posedness of the test cases}{{2.3}{37}{Well-posedness of the test cases}{section.2.3}{}}
\citation{CR97}
\citation{QMN15}
\citation{ESW04}
\citation{Qua10}
\citation{Rud64}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Finite element method}{38}{section.2.4}}
\newlabel{section:Finite element method}{{2.4}{38}{Finite element method}{section.2.4}{}}
\newlabel{eq:galerkin}{{2.43}{38}{Finite element method}{equation.2.43}{}}
\newlabel{eq:continuity}{{2.44}{39}{Finite element method}{equation.2.44}{}}
\newlabel{eq:inf-sup}{{2.45}{39}{Finite element method}{equation.2.45}{}}
\newlabel{eq:newton-linearized-problem}{{2.46}{39}{Finite element method}{equation.2.46}{}}
\newlabel{eq:galerkin-solution}{{2.47}{39}{Finite element method}{equation.2.47}{}}
\newlabel{eq:galerkin-algebraic}{{2.48}{39}{Finite element method}{equation.2.48}{}}
\newlabel{eq:galerkin-nonlinear-system}{{2.49}{39}{Finite element method}{equation.2.49}{}}
\newlabel{eq:galerkin-nonlinear-system-equation}{{2.50}{39}{Finite element method}{equation.2.50}{}}
\newlabel{eq:galerkin-linear-system}{{2.51}{39}{Finite element method}{equation.2.51}{}}
\citation{QMN15}
\newlabel{eq:notation-1}{{2.53}{40}{Finite element method}{equation.2.53}{}}
\newlabel{eq:notation-2}{{2.54}{40}{Finite element method}{equation.2.54}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces The Newton's method applied to the nonlinear system \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:galerkin-nonlinear-system}\unskip \@@italiccorr )}}.\relax }}{41}{algorithm.2.1}}
\newlabel{alg:newton-full}{{2.1}{41}{The Newton's method applied to the nonlinear system \eqref {eq:galerkin-nonlinear-system}.\relax }{algorithm.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Nonlinear Poisson equation}{41}{subsection.2.4.1}}
\newlabel{section:Nonlinear Poisson equation (FE)}{{2.4.1}{41}{Nonlinear Poisson equation}{subsection.2.4.1}{}}
\newlabel{eq:poisson-residual-vector}{{2.4.1}{41}{Nonlinear Poisson equation}{subsection.2.4.1}{}}
\newlabel{eq:poisson-jacobian}{{2.4.1}{41}{Nonlinear Poisson equation}{subsection.2.4.1}{}}
\citation{Ran99}
\citation{Qua10}
\citation{Per02}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Steady Navier-Stokes equations}{42}{subsection.2.4.2}}
\newlabel{section:Steady Navier-Stokes equations (FE)}{{2.4.2}{42}{Steady Navier-Stokes equations}{subsection.2.4.2}{}}
\newlabel{eq:inf-sup-ns}{{2.55}{42}{Steady Navier-Stokes equations}{equation.2.55}{}}
\newlabel{eq:B}{{2.57c}{43}{Steady Navier-Stokes equations}{equation.2.57c}{}}
\newlabel{eq:ns-system}{{2.58}{43}{Steady Navier-Stokes equations}{equation.2.58}{}}
\citation{Dep08}
\citation{HSR16}
\citation{QMN15}
\citation{HSR16}
\citation{Chen17}
\citation{Vol08}
\newlabel{eq:ns-jacobian}{{2.59}{44}{Steady Navier-Stokes equations}{equation.2.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}POD-Galerkin reduced basis method}{44}{section.2.5}}
\newlabel{section:POD-Galerkin reduced basis method}{{2.5}{44}{POD-Galerkin reduced basis method}{section.2.5}{}}
\citation{HSR16}
\citation{Mad06}
\citation{Mad06}
\citation{Buf12}
\citation{Buf12}
\citation{HSR16}
\citation{Mad06}
\citation{Dep08}
\citation{QMN15}
\newlabel{eq:rb-solution}{{2.60}{45}{POD-Galerkin reduced basis method}{equation.2.60}{}}
\newlabel{eq:kolmogorov-L-width}{{2.62}{45}{}{equation.2.62}{}}
\newlabel{eq:pde-rb}{{2.63}{46}{POD-Galerkin reduced basis method}{equation.2.63}{}}
\newlabel{eq:pde-rb-newton}{{2.64}{46}{POD-Galerkin reduced basis method}{equation.2.64}{}}
\newlabel{eq:rb-fe-coefficients}{{2.65}{46}{POD-Galerkin reduced basis method}{equation.2.65}{}}
\citation{QMN15}
\citation{Vol08}
\citation{Lia02}
\newlabel{eq:rb-algebraic-formulation-1}{{2.66}{47}{POD-Galerkin reduced basis method}{equation.2.66}{}}
\newlabel{eq:rb-nonlinear-system}{{2.67}{47}{POD-Galerkin reduced basis method}{equation.2.67}{}}
\newlabel{eq:rb-nonlinear-system-jacobian}{{2.68}{47}{POD-Galerkin reduced basis method}{equation.2.68}{}}
\newlabel{eq:rb-nonlinear-system-newton}{{2.69}{47}{POD-Galerkin reduced basis method}{equation.2.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Proper Orthogonal Decomposition}{47}{subsection.2.5.1}}
\newlabel{section:Proper Orthogonal Decomposition}{{2.5.1}{47}{Proper Orthogonal Decomposition}{subsection.2.5.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.2}{\ignorespaces The Newton's method applied to the reduced nonlinear system \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:rb-nonlinear-system}\unskip \@@italiccorr )}}.\relax }}{48}{algorithm.2.2}}
\newlabel{alg:newton-rb}{{2.2}{48}{The Newton's method applied to the reduced nonlinear system \eqref {eq:rb-nonlinear-system}.\relax }{algorithm.2.2}{}}
\newlabel{eq:svd}{{2.70}{48}{Proper Orthogonal Decomposition}{equation.2.70}{}}
\citation{Vol08}
\newlabel{eq:left-singular-vectors}{{2.71}{49}{Proper Orthogonal Decomposition}{equation.2.71}{}}
\newlabel{eq:right-singular-vectors}{{2.72}{49}{Proper Orthogonal Decomposition}{equation.2.72}{}}
\newlabel{eq:snapshot-method}{{2.73}{49}{Proper Orthogonal Decomposition}{equation.2.73}{}}
\newlabel{eq:svd-compact}{{2.75}{49}{Proper Orthogonal Decomposition}{equation.2.75}{}}
\newlabel{eq:svd-compact-matrices}{{2.76}{49}{Proper Orthogonal Decomposition}{equation.2.76}{}}
\newlabel{eq:basis-error}{{2.77}{49}{Proper Orthogonal Decomposition}{equation.2.77}{}}
\citation{Vol08}
\citation{Vol08}
\newlabel{eq:pod-optimality}{{2.78}{50}{Schmidt-Eckart-Young}{equation.2.78}{}}
\citation{HSR16}
\citation{Bur06}
\citation{QMN15}
\newlabel{eq:pod-error}{{2.79}{51}{Proper Orthogonal Decomposition}{equation.2.79}{}}
\newlabel{eq:pod-fe-basis-functions}{{2.81}{51}{Proper Orthogonal Decomposition}{equation.2.81}{}}
\newlabel{eq:discrete-scalar-product}{{2.82}{51}{Proper Orthogonal Decomposition}{equation.2.82}{}}
\citation{Pru02}
\newlabel{eq:relative-information-content}{{2.83}{52}{Proper Orthogonal Decomposition}{equation.2.83}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.3}{\ignorespaces The POD algorithm.\relax }}{52}{algorithm.2.3}}
\newlabel{alg:pod}{{2.3}{52}{The POD algorithm.\relax }{algorithm.2.3}{}}
\citation{Bar04}
\citation{MN16}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Implementation: details and issues}{53}{subsection.2.5.2}}
\newlabel{section:Implementation}{{2.5.2}{53}{Implementation: details and issues}{subsection.2.5.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.4}{\ignorespaces The offline and online stages for the POD-Galerkin (POD-G) RB method.\relax }}{53}{algorithm.2.4}}
\newlabel{alg:pod-galerkin}{{2.4}{53}{The offline and online stages for the POD-Galerkin (POD-G) RB method.\relax }{algorithm.2.4}{}}
\citation{QMN15}
\citation{Bal14}
\citation{Bur06}
\citation{Chen17}
\citation{QMN15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Application to the steady Navier-Stokes equations}{54}{subsection.2.5.3}}
\newlabel{section:Application to the steady Navier-Stokes equations}{{2.5.3}{54}{Application to the steady Navier-Stokes equations}{subsection.2.5.3}{}}
\citation{Bur06}
\citation{Bal14}
\newlabel{eq:ns-reduced-system}{{2.84}{55}{Application to the steady Navier-Stokes equations}{equation.2.84}{}}
\newlabel{eq:supremizer-system}{{2.85}{55}{Application to the steady Navier-Stokes equations}{equation.2.85}{}}
\citation{Bal14}
\citation{Bar04}
\newlabel{eq:inf-sup-ns-reduced}{{2.87}{56}{}{equation.2.87}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}A POD-based RB method using neural networks}{56}{section.2.6}}
\newlabel{section:A POD-based RB method using neural networks}{{2.6}{56}{A POD-based RB method using neural networks}{section.2.6}{}}
\citation{Imam08}
\newlabel{eq:good-rb-solution}{{2.88}{57}{A POD-based RB method using neural networks}{equation.2.88}{}}
\newlabel{eq:nonlinear-poisson-example}{{2.89}{57}{A POD-based RB method using neural networks}{equation.2.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The parametrized computational domain (solid line) for the Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:nonlinear-poisson-example}\unskip \@@italiccorr )}}.\relax }}{58}{figure.caption.17}}
\newlabel{fig:nonlinear-poisson-example-domain}{{2.3}{58}{The parametrized computational domain (solid line) for the Poisson problem \eqref {eq:nonlinear-poisson-example}.\relax }{figure.caption.17}{}}
\newlabel{eq:high-fidelity-projected}{{2.90}{58}{A POD-based RB method using neural networks}{equation.2.90}{}}
\citation{Ams10}
\citation{Chen17}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: average relative error between the FE solution for the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:nonlinear-poisson-example}\unskip \@@italiccorr )}} and either its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal  {P}$ consisting of $N_{te} = 50$ randomly picked values. Right: comparison between the online run times for the FE (full-order) scheme and the POD-Galerkin (reduced-order) method on $\Xi _{te}$.\relax }}{59}{figure.caption.18}}
\newlabel{fig:nonlinear-poisson-example}{{2.4}{59}{Left: average relative error between the FE solution for the problem \eqref {eq:nonlinear-poisson-example} and either its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal {P}$ consisting of $N_{te} = 50$ randomly picked values. Right: comparison between the online run times for the FE (full-order) scheme and the POD-Galerkin (reduced-order) method on $\Xi _{te}$.\relax }{figure.caption.18}{}}
\newlabel{eq:map-to-approximate}{{2.91}{59}{A POD-based RB method using neural networks}{equation.2.91}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.5}{\ignorespaces Selection of an optimal network configuration.\relax }}{61}{algorithm.2.5}}
\newlabel{alg:podnn-training}{{2.5}{61}{Selection of an optimal network configuration.\relax }{algorithm.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Three point sets randomly generate through the latin hypercube sampling. Observe the good coverage provided by the union of the sets, with only a few overlapping points.\relax }}{61}{figure.caption.19}}
\newlabel{fig:lhs}{{2.5}{61}{Three point sets randomly generate through the latin hypercube sampling. Observe the good coverage provided by the union of the sets, with only a few overlapping points.\relax }{figure.caption.19}{}}
\newlabel{eq:pod-nn-mse}{{2.94}{62}{A POD-based RB method using neural networks}{equation.2.94}{}}
\newlabel{eq:pod-nn-solution}{{2.95}{62}{A POD-based RB method using neural networks}{equation.2.95}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.6}{\ignorespaces The offline and online stages for the POD-NN RB method.\relax }}{62}{algorithm.2.6}}
\newlabel{alg:pod-nn}{{2.6}{62}{The offline and online stages for the POD-NN RB method.\relax }{algorithm.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}An a priori error analysis}{63}{subsection.2.6.1}}
\newlabel{section:An a priori error analysis}{{2.6.1}{63}{An a priori error analysis}{subsection.2.6.1}{}}
\newlabel{eq:error-analysis-1}{{2.96}{63}{An a priori error analysis}{equation.2.96}{}}
\newlabel{eq:error-analysis-2}{{2.97}{63}{An a priori error analysis}{equation.2.97}{}}
\newlabel{eq:error-analysis-3}{{2.99}{64}{An a priori error analysis}{equation.2.99}{}}
\newlabel{eq:error-analysis-4}{{2.100}{64}{An a priori error analysis}{equation.2.100}{}}
\newlabel{eq:error-analysis-5}{{2.101}{64}{An a priori error analysis}{equation.2.101}{}}
\newlabel{eq:error-analysis-final}{{2.102}{65}{An a priori error analysis}{equation.2.102}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Numerical results}{67}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Numerical results}{{3}{67}{Numerical results}{chapter.3}{}}
\newlabel{eq:podg-error}{{3.1}{67}{Numerical results}{equation.3.1}{}}
\newlabel{eq:podnn-error}{{3.2}{67}{Numerical results}{equation.3.2}{}}
\newlabel{eq:projection-error}{{3.3}{68}{Numerical results}{equation.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}One-dimensional Poisson equation}{69}{section.3.1}}
\newlabel{section:One-dimensional Poisson equation (results)}{{3.1}{69}{One-dimensional Poisson equation}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Linear test case}{69}{subsection.3.1.1}}
\newlabel{section:poisson1d-1}{{3.1.1}{69}{Linear test case}{subsection.3.1.1}{}}
\newlabel{eq:poisson1d-1}{{3.4}{69}{Linear test case}{equation.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: error analysis for the POD-G RB method applied to Equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-1}\unskip \@@italiccorr )}}; the results refer to a POD basis constructed based on $N = 25$, $50$, $100$ snapshots; for $N = 100$, the singular values of the correspondent snapshot matrix are shown as well. Right: FE and POD-G solutions for three parameter values.\relax }}{69}{figure.caption.20}}
\newlabel{fig:poisson1d-1-fig1}{{3.1}{69}{Left: error analysis for the POD-G RB method applied to Equation \eqref {eq:poisson1d-1}; the results refer to a POD basis constructed based on $N = 25$, $50$, $100$ snapshots; for $N = 100$, the singular values of the correspondent snapshot matrix are shown as well. Right: FE and POD-G solutions for three parameter values.\relax }{figure.caption.20}{}}
\citation{Mol93}
\citation{GMW81}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces In respect to the approximation of the map \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:map-to-approximate}\unskip \@@italiccorr )}} for the Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-1}\unskip \@@italiccorr )}} via neural networks, comparison between three supervised algorithms - Levenberg-Marquardt (blue), Scaled Conjugate Gradient (orange), BFGS Quasi-Newton (yellow) - in terms of the optimal configuration detected (\emph  {right}) and the associated test error (\emph  {right}) for different amounts of training samples.\relax }}{70}{figure.caption.21}}
\newlabel{fig:poisson1d-1-fig2}{{3.2}{70}{In respect to the approximation of the map \eqref {eq:map-to-approximate} for the Poisson problem \eqref {eq:poisson1d-1} via neural networks, comparison between three supervised algorithms - Levenberg-Marquardt (blue), Scaled Conjugate Gradient (orange), BFGS Quasi-Newton (yellow) - in terms of the optimal configuration detected (\emph {right}) and the associated test error (\emph {right}) for different amounts of training samples.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Left: relative error yielded by the POD-NN RB method applied to Equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-1}\unskip \@@italiccorr )}}; the convergence to the projection error for $L = 10$ (solid line) is analyzed in terms of both the number of neurons included in the neural network and the dimension of the training set. Right: FE and POD-NN solutions for three parameter values.\relax }}{70}{figure.caption.21}}
\newlabel{fig:poisson1d-1-fig3}{{3.3}{70}{Left: relative error yielded by the POD-NN RB method applied to Equation \eqref {eq:poisson1d-1}; the convergence to the projection error for $L = 10$ (solid line) is analyzed in terms of both the number of neurons included in the neural network and the dimension of the training set. Right: FE and POD-NN solutions for three parameter values.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Nonlinear test case, two parameters}{71}{subsection.3.1.2}}
\newlabel{section:poisson1d-2}{{3.1.2}{71}{Nonlinear test case, two parameters}{subsection.3.1.2}{}}
\newlabel{eq:poisson1d-2}{{3.5}{71}{Nonlinear test case, two parameters}{equation.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Error analysis for the POD-Galerkin RB method applied to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-2}\unskip \@@italiccorr )}}. The lower-bound provided by the projection error \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:projection-error}\unskip \@@italiccorr )}} is reported as reference.\relax }}{71}{figure.caption.22}}
\newlabel{fig:poisson1d-2-fig1}{{3.4}{71}{Error analysis for the POD-Galerkin RB method applied to the problem \eqref {eq:poisson1d-2}. The lower-bound provided by the projection error \eqref {eq:projection-error} is reported as reference.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Left: error analysis for the POD-NN RB method (dotted lines) applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-2}\unskip \@@italiccorr )}}. The solid solid sections refer to the steps carried out by the routine \ref  {alg:podnn-training}; the error yielded by the POD-G method using $L = 8$ or $L = 15$ basis functions are reported as reference. Right: comparison between FE and POD-NN solutions for three input vectors.\relax }}{72}{figure.caption.23}}
\newlabel{fig:poisson1d-2-fig2}{{3.5}{72}{Left: error analysis for the POD-NN RB method (dotted lines) applied to problem \eqref {eq:poisson1d-2}. The solid solid sections refer to the steps carried out by the routine \ref {alg:podnn-training}; the error yielded by the POD-G method using $L = 8$ or $L = 15$ basis functions are reported as reference. Right: comparison between FE and POD-NN solutions for three input vectors.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces From left to right, top to bottom: regression plots for the first, seventh, eighth and nine scalar components of the outputs provided on $\Xi _{te}$ by a network with $H_1 = H_2 = 25$ neurons per hidden layer, trained to approximate the map \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:map-to-approximate}\unskip \@@italiccorr )}} for problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-2}\unskip \@@italiccorr )}}.\relax }}{73}{figure.caption.24}}
\newlabel{fig:poisson1d-2-fig3}{{3.6}{73}{From left to right, top to bottom: regression plots for the first, seventh, eighth and nine scalar components of the outputs provided on $\Xi _{te}$ by a network with $H_1 = H_2 = 25$ neurons per hidden layer, trained to approximate the map \eqref {eq:map-to-approximate} for problem \eqref {eq:poisson1d-2}.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Nonlinear test case, three parameters}{73}{subsection.3.1.3}}
\newlabel{section:poisson1d-3}{{3.1.3}{73}{Nonlinear test case, three parameters}{subsection.3.1.3}{}}
\newlabel{eq:poisson1d-3}{{3.6}{73}{Nonlinear test case, three parameters}{equation.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The first $10$ POD basis functions for the (nonlinear) Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-3}\unskip \@@italiccorr )}}.\relax }}{74}{figure.caption.25}}
\newlabel{fig:poisson1d-3-fig1}{{3.7}{74}{The first $10$ POD basis functions for the (nonlinear) Poisson problem \eqref {eq:poisson1d-3}.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\psi _1$}}}{74}{subfigure.7.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\psi _2$}}}{74}{subfigure.7.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\psi _3$}}}{74}{subfigure.7.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\psi _4$}}}{74}{subfigure.7.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {$\psi _5$}}}{74}{subfigure.7.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {$\psi _6$}}}{74}{subfigure.7.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {$\psi _7$}}}{74}{subfigure.7.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {$\psi _8$}}}{74}{subfigure.7.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {$\psi _9$}}}{74}{subfigure.7.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {$\psi _{10}$}}}{74}{subfigure.7.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Convergence analysis for the POD-G and POD-NN methods applied to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-3}\unskip \@@italiccorr )}} (\emph  {left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph  {right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer, and considering $L = 10$ POD modes.\relax }}{74}{figure.caption.26}}
\newlabel{fig:poisson1d-3-fig2}{{3.8}{74}{Convergence analysis for the POD-G and POD-NN methods applied to the problem \eqref {eq:poisson1d-3} (\emph {left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph {right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer, and considering $L = 10$ POD modes.\relax }{figure.caption.26}{}}
\newlabel{eq:podnn-solution-full}{{3.7}{75}{Nonlinear test case, three parameters}{equation.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Sensitivity analysis for the POD-NN method applied to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-3}\unskip \@@italiccorr )}} with respect to the number of POD coefficients (per sample) used during the training; to prevent overfitting, $N_{tr} = 300$ learning patterns have been used.\relax }}{75}{figure.caption.27}}
\newlabel{fig:poisson1d-3-fig3}{{3.9}{75}{Sensitivity analysis for the POD-NN method applied to the problem \eqref {eq:poisson1d-3} with respect to the number of POD coefficients (per sample) used during the training; to prevent overfitting, $N_{tr} = 300$ learning patterns have been used.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Two-dimensional Poisson equation}{75}{section.3.2}}
\newlabel{section:Two-dimensional Poisson equation (results)}{{3.2}{75}{Two-dimensional Poisson equation}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The quadrilateral domain used in the simulations (\emph  {left}, solid line) and the parametrizations of its sides (\emph  {right}).\relax }}{76}{figure.caption.28}}
\newlabel{fig:quadrilateral-domain}{{3.10}{76}{The quadrilateral domain used in the simulations (\emph {left}, solid line) and the parametrizations of its sides (\emph {right}).\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The stenosis geometry employed in the simulations (\emph  {left}) and the parametrizations of its sides (\emph  {right}).\relax }}{76}{figure.caption.28}}
\newlabel{fig:stenosis-domain}{{3.11}{76}{The stenosis geometry employed in the simulations (\emph {left}) and the parametrizations of its sides (\emph {right}).\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Linear test case}{77}{subsection.3.2.1}}
\newlabel{section:poisson2d-1}{{3.2.1}{77}{Linear test case}{subsection.3.2.1}{}}
\newlabel{eq:poisson2d-1}{{3.8}{77}{Linear test case}{equation.3.8}{}}
\newlabel{eq:poisson2d-1-fe-system}{{3.9}{77}{Linear test case}{equation.3.9}{}}
\newlabel{eq:poisson2d-1-podg-system}{{3.10}{77}{Linear test case}{equation.3.10}{}}
\newlabel{eq:poisson2d-1-podg-matrices}{{3.11}{77}{Linear test case}{equation.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Online run times for the FE, POD-G and POD-NN methods applied to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-1}\unskip \@@italiccorr )}}, for $N_{te} = 100$ randomly generated parameter values. \relax }}{78}{figure.caption.29}}
\newlabel{fig:poisson2d-1-fig1}{{3.12}{78}{Online run times for the FE, POD-G and POD-NN methods applied to the problem \eqref {eq:poisson2d-1}, for $N_{te} = 100$ randomly generated parameter values. \relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces On the right, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods applied to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-1}\unskip \@@italiccorr )}}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the left. The solid tracts denote the steps actually pursued by the automatic training routine \ref  {alg:podnn-training}. All the results refer to neural networks exposed to $L = 15$ POD coefficients per learning sample.\relax }}{78}{figure.caption.30}}
\newlabel{fig:poisson2d-1-fig2}{{3.13}{78}{On the right, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods applied to the problem \eqref {eq:poisson2d-1}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the left. The solid tracts denote the steps actually pursued by the automatic training routine \ref {alg:podnn-training}. All the results refer to neural networks exposed to $L = 15$ POD coefficients per learning sample.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Nonlinear test case, two parameters}{79}{subsection.3.2.2}}
\newlabel{section:poisson2d-2}{{3.2.2}{79}{Nonlinear test case, two parameters}{subsection.3.2.2}{}}
\newlabel{eq:poisson2d-2}{{3.12}{79}{Nonlinear test case, two parameters}{equation.3.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Finite element (\emph  {left}), POD-G (\emph  {center}) and POD-NN (\emph  {right}) solution to the semilinear Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-2}\unskip \@@italiccorr )}} re-stated over the reference domain, with $\bm  {\mu } = (0.835, \tmspace  +\thinmuskip {.1667em} 0.034)$.\relax }}{79}{figure.caption.31}}
\newlabel{fig:poisson2d-2-fig1}{{3.14}{79}{Finite element (\emph {left}), POD-G (\emph {center}) and POD-NN (\emph {right}) solution to the semilinear Poisson problem \eqref {eq:poisson2d-2} re-stated over the reference domain, with $\bg {\mu } = (0.835, \, 0.034)$.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Online relative errors (\emph  {left}) and run times (\emph  {right}) for the POD-G and the POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-2}\unskip \@@italiccorr )}} for $N_{te} = 50$ randomly picked parameter values.\relax }}{80}{figure.caption.32}}
\newlabel{fig:poisson2d-2-fig2}{{3.15}{80}{Online relative errors (\emph {left}) and run times (\emph {right}) for the POD-G and the POD-NN methods applied to problem \eqref {eq:poisson2d-2} for $N_{te} = 50$ randomly picked parameter values.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Convergence analysis for the POD-NN RB method applied to the BVP \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-2}\unskip \@@italiccorr )}}. The results have been obtained via three-layers (i.e., two hidden layers plus the output layer) neural networks with $H_1 = H_2 \in {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }15, \tmspace  +\thinmuskip {.1667em} 20, \tmspace  +\thinmuskip {.1667em} 25, \tmspace  +\thinmuskip {.1667em} 30, \tmspace  +\thinmuskip {.1667em} 35 {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ neurons in each hidden layer, and trained with $N_{tr} \in {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }100, \tmspace  +\thinmuskip {.1667em} 200, \tmspace  +\thinmuskip {.1667em} 300 {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ learning patterns. Each pattern consists of an input vector $\bm  {\mu } \in \mathcal  {P}$ and the first $L = 35$ POD coefficients ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } \mathbf  {u}_h(\bm  {\mu }), \bm  {\psi }_i {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }_{\math@bb  {R}^M}$, $i = 1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , L$, as teaching inputs.\relax }}{80}{figure.caption.33}}
\newlabel{fig:poisson2d-2-fig3}{{3.16}{80}{Convergence analysis for the POD-NN RB method applied to the BVP \eqref {eq:poisson2d-2}. The results have been obtained via three-layers (i.e., two hidden layers plus the output layer) neural networks with $H_1 = H_2 \in \big \lbrace 15, \, 20, \, 25, \, 30, \, 35 \big \rbrace $ neurons in each hidden layer, and trained with $N_{tr} \in \big \lbrace 100, \, 200, \, 300 \big \rbrace $ learning patterns. Each pattern consists of an input vector $\bg {\mu } \in \mathcal {P}$ and the first $L = 35$ POD coefficients $\big ( \mathbf {u}_h(\bg {\mu }), \bg {\psi }_i \big )_{\mathbb {R}^M}$, $i = 1, \, \ldots \, , L$, as teaching inputs.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Nonlinear test case, three parameters}{80}{subsection.3.2.3}}
\newlabel{section:poisson2d-3}{{3.2.3}{80}{Nonlinear test case, three parameters}{subsection.3.2.3}{}}
\newlabel{eq:poisson2d-3}{{3.13}{81}{Nonlinear test case, three parameters}{equation.3.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces On the left, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods (\emph  {left}) applied to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-3}\unskip \@@italiccorr )}}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the right. The solid tracts denote the steps actually pursued by the automatic training routine \ref  {alg:podnn-training}. All the results refer to neural networks exposed to $L = 30$ POD coefficients per sample during the learning step.\relax }}{81}{figure.caption.34}}
\newlabel{}{{3.17}{81}{On the left, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods (\emph {left}) applied to the problem \eqref {eq:poisson2d-3}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the right. The solid tracts denote the steps actually pursued by the automatic training routine \ref {alg:podnn-training}. All the results refer to neural networks exposed to $L = 30$ POD coefficients per sample during the learning step.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces \relax }}{82}{figure.caption.35}}
\newlabel{}{{3.18}{82}{\relax }{figure.caption.35}{}}
\bibcite{Ams10}{1}
\bibcite{Bal14}{2}
\bibcite{Bar04}{3}
\bibcite{Buf12}{4}
\bibcite{Bur06}{5}
\bibcite{CR97}{6}
\bibcite{Chen17}{7}
\bibcite{Cyb88}{8}
\bibcite{Cyb89}{9}
\bibcite{Dep08}{10}
\bibcite{ESW04}{11}
\bibcite{Fah88}{12}
\bibcite{GMW81}{13}
\bibcite{Hag94}{14}
\bibcite{Hag14}{15}
\bibcite{Haa13}{16}
\bibcite{Hay05}{17}
\bibcite{Heb49}{18}
\bibcite{Lia02}{19}
\bibcite{OBS}{20}
\bibcite{HSR16}{21}
\bibcite{Hop82}{22}
\bibcite{Imam08}{23}
\bibcite{JIR14}{24}
\bibcite{KLM96}{25}
\bibcite{Koh95}{26}
\bibcite{Koh98}{27}
\bibcite{Kri}{28}
\bibcite{Mad06}{29}
\bibcite{Mar63}{30}
\bibcite{Mat16}{31}
\bibcite{MM10}{32}
\bibcite{MN16}{33}
\bibcite{Mol93}{34}
\bibcite{MR86}{35}
\bibcite{Nie15}{36}
\bibcite{NMA15}{37}
\bibcite{Per02}{38}
\bibcite{Pru02}{39}
\bibcite{Qua10}{40}
\bibcite{QMN15}{41}
\bibcite{Ran99}{42}
\bibcite{RB93}{43}
\bibcite{Ros58}{44}
\bibcite{Rud64}{45}
\bibcite{SD13}{46}
\bibcite{Vol08}{47}
\bibcite{WH60}{48}
