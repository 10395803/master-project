\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\citation{Kri}
\citation{Hag14}
\citation{Hay05}
\citation{Kri}
\citation{Kri}
\citation{Kri}
\citation{Hay05}
\citation{Kri}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to neural networks}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Introduction to neural networks}{{1}{1}{Introduction to neural networks}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Biological motivation}{1}{section.1.1}}
\newlabel{section:Biological motivation}{{1.1}{1}{Biological motivation}{section.1.1}{}}
\citation{SD13}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite  {Kri}.\relax }}{2}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neuron}{{1.1}{2}{Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite {Kri}.\relax }{figure.caption.6}{}}
\citation{Hag14}
\citation{Kri}
\citation{Kri}
\citation{Nie15}
\citation{SD13}
\citation{Kri}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Artificial neural networks}{3}{section.1.2}}
\newlabel{section:Artificial neural networks}{{1.2}{3}{Artificial neural networks}{section.1.2}{}}
\newlabel{def:neural-network}{{1.1}{3}{Neural network}{definition.1.1}{}}
\citation{Kri}
\citation{Kri}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace  {} y_{s_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace  {} y_{s_m} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , s_m$, and fires $y_j$, sent to the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. The neuron threshold $\theta _j$ is reported within its body.\relax }}{4}{figure.caption.7}}
\newlabel{fig:neural-model}{{1.2}{4}{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big \lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m} \big \rbrace $ coming from the sending neurons $s_1, \, \ldots \, , s_m$, and fires $y_j$, sent to the target neurons $\big \lbrace r_1, \, \ldots \, , r_n \big \rbrace $ through the synapsis $\big \lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big \rbrace $. The neuron threshold $\theta _j$ is reported within its body.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Neuronal model}{4}{subsection.1.2.1}}
\newlabel{section:Neuronal model}{{1.2.1}{4}{Neuronal model}{subsection.1.2.1}{}}
\newlabel{eq:propagation-function}{{1.1}{4}{Neuronal model}{equation.1.1}{}}
\newlabel{eq:weighted-sum}{{1.2}{4}{Neuronal model}{equation.1.2}{}}
\newlabel{eq:activation-function}{{1.3}{4}{Neuronal model}{equation.1.3}{}}
\citation{Hay05}
\citation{Hay05}
\newlabel{eq:net-input}{{1.4}{5}{Neuronal model}{equation.1.4}{}}
\newlabel{eq:heaviside}{{1.6}{5}{Neuronal model}{equation.1.6}{}}
\newlabel{eq:logistic}{{1.7}{5}{Neuronal model}{equation.1.7}{}}
\citation{Hay05}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace  {} y_{s_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace  {} y_{s_m}, \tmspace  +\thinmuskip {.1667em} -\theta _j {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , s_m, \tmspace  +\thinmuskip {.1667em} b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. Observe that, conversely to the model offered in Figure \ref  {fig:neural-model}, the neuron threshold is now set to $0$.\relax }}{6}{figure.caption.8}}
\newlabel{fig:neural-model-bias}{{1.3}{6}{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big \lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m}, \, -\theta _j \big \rbrace $ coming from the sending neurons $s_1, \, \ldots \, , s_m, \, b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons $\big \lbrace r_1, \, \ldots \, , r_n \big \rbrace $ through the synapsis $\big \lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big \rbrace $. Observe that, conversely to the model offered in Figure \ref {fig:neural-model}, the neuron threshold is now set to $0$.\relax }{figure.caption.8}{}}
\newlabel{eq:hyperbolic-tangent}{{1.8}{6}{Neuronal model}{equation.1.8}{}}
\newlabel{eq:output-function}{{1.9}{6}{Neuronal model}{equation.1.9}{}}
\citation{Ros58}
\citation{Kri}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Left: logistic function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logistic}\unskip \@@italiccorr )}} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resemble the Heaviside function. Right: hyperbolic tangent.\relax }}{7}{figure.caption.9}}
\newlabel{fig:activation-functions}{{1.4}{7}{Left: logistic function \eqref {eq:logistic} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resemble the Heaviside function. Right: hyperbolic tangent.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Network topologies: the feedforward neural network}{7}{subsection.1.2.2}}
\newlabel{section:Network topologies}{{1.2.2}{7}{Network topologies: the feedforward neural network}{subsection.1.2.2}{}}
\citation{Cyb88}
\citation{Cyb89}
\citation{Cyb89}
\citation{Cyb88}
\citation{Hop82}
\citation{Koh98}
\newlabel{cybenko-first-rule}{{{{(i)}}}{8}{Network topologies: the feedforward neural network}{Item.1}{}}
\newlabel{cybenko-second-rule}{{{{(ii)}}}{8}{Network topologies: the feedforward neural network}{Item.2}{}}
\citation{Hay05}
\citation{Kri}
\citation{Kri}
\citation{KLM96}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Training a multilayer feedforward neural network}{9}{subsection.1.2.3}}
\newlabel{section:Training a multilayer feedforard neural network}{{1.2.3}{9}{Training a multilayer feedforward neural network}{subsection.1.2.3}{}}
\citation{Heb49}
\citation{Hay05}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }}{10}{figure.caption.10}}
\newlabel{fig:neural-network}{{1.5}{10}{A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }{figure.caption.10}{}}
\newlabel{a}{{{{(a)}}}{10}{Training a multilayer feedforward neural network}{Item.3}{}}
\newlabel{b}{{{{(b)}}}{10}{Training a multilayer feedforward neural network}{Item.4}{}}
\citation{Hay05}
\citation{Kri}
\newlabel{first-rule}{{{{(i)}}}{11}{Training a multilayer feedforward neural network}{Item.5}{}}
\newlabel{second-rule}{{{{(ii)}}}{11}{Training a multilayer feedforward neural network}{Item.6}{}}
\newlabel{first-rule}{{1.2.3}{11}{Training a multilayer feedforward neural network}{Item.6}{}}
\newlabel{eq:hebbian-rule}{{1.10}{11}{Training a multilayer feedforward neural network}{equation.1.10}{}}
\newlabel{eq:weight-update}{{1.11}{11}{Training a multilayer feedforward neural network}{equation.1.11}{}}
\newlabel{eq:generalized-hebbian-rule}{{1.12}{11}{Training a multilayer feedforward neural network}{equation.1.12}{}}
\newlabel{eq:performance-function}{{1.13}{11}{Training a multilayer feedforward neural network}{equation.1.13}{}}
\citation{Hay05}
\citation{MR86}
\newlabel{eq:accumulated-error}{{1.14}{12}{Training a multilayer feedforward neural network}{equation.1.14}{}}
\newlabel{eq:mse}{{1.15}{12}{Training a multilayer feedforward neural network}{equation.1.15}{}}
\newlabel{eq:accumulated-mse}{{1.16}{12}{Training a multilayer feedforward neural network}{equation.1.16}{}}
\newlabel{eq:antigradient}{{1.17}{12}{Training a multilayer feedforward neural network}{equation.1.17}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.1}{\ignorespaces Backbone of any supervised online learning algorithm; note that the full procedure ends when all training patterns yield an error which is below a defined threshold.\relax }}{13}{algorithm.1.1}}
\newlabel{alg:online-learning}{{1.1}{13}{Backbone of any supervised online learning algorithm; note that the full procedure ends when all training patterns yield an error which is below a defined threshold.\relax }{algorithm.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation of error}{13}{section*.11}}
\newlabel{section:Backpropagation of error}{{1.2.3}{13}{Backpropagation of error}{section*.11}{}}
\newlabel{eq:bp-first-equation}{{1.18}{13}{Backpropagation of error}{equation.1.18}{}}
\newlabel{eq:bp-second-equation}{{1.19}{13}{Backpropagation of error}{equation.1.19}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.2}{\ignorespaces Backbone of any supervised offline learning algorithm; the procedure to compute the accumulated error is provided as well.\relax }}{14}{algorithm.1.2}}
\newlabel{alg:offline-learning}{{1.2}{14}{Backbone of any supervised offline learning algorithm; the procedure to compute the accumulated error is provided as well.\relax }{algorithm.1.2}{}}
\newlabel{eq:bp-third-equation}{{1.20}{14}{Backpropagation of error}{equation.1.20}{}}
\newlabel{eq:delta}{{1.21}{14}{Backpropagation of error}{equation.1.21}{}}
\newlabel{eq:bp-weight-update}{{1.22}{14}{Backpropagation of error}{equation.1.22}{}}
\citation{Kri}
\newlabel{eq:bp-fourth-equation}{{1.23}{15}{Backpropagation of error}{equation.1.23}{}}
\newlabel{eq:bp-fifth-equation}{{1.24}{15}{Backpropagation of error}{equation.1.24}{}}
\newlabel{eq:bp-sixth-equation}{{1.25}{15}{Backpropagation of error}{equation.1.25}{}}
\citation{Kri}
\citation{WH60}
\citation{Kri}
\citation{Kri}
\citation{RB93}
\citation{Fah88}
\citation{Mar63}
\citation{Hag94}
\newlabel{eq:bp-inner-neuron}{{1.26a}{16}{Backpropagation of error}{equation.1.26}{}}
\newlabel{eq:bp-output-neuron}{{1.26b}{16}{Backpropagation of error}{equation.1.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{Levenberg-Marquardt algorithm}{16}{section*.12}}
\newlabel{section:Levenberg-Marquardt algorithm}{{1.2.3}{16}{Levenberg-Marquardt algorithm}{section*.12}{}}
\citation{Hag94}
\citation{Mar63}
\citation{Mar63}
\newlabel{eq:newton}{{1.27}{17}{Levenberg-Marquardt algorithm}{equation.1.27}{}}
\newlabel{eq:accumulated-mse-bis}{{1.28}{17}{Levenberg-Marquardt algorithm}{equation.1.28}{}}
\newlabel{eq:jacobian}{{1.29}{17}{Levenberg-Marquardt algorithm}{equation.1.29}{}}
\newlabel{eq:gradient}{{1.30}{17}{Levenberg-Marquardt algorithm}{equation.1.30}{}}
\newlabel{eq:hessian}{{1.31}{17}{Levenberg-Marquardt algorithm}{equation.1.31}{}}
\newlabel{eq:newton-quadratic-function}{{1.32}{17}{Levenberg-Marquardt algorithm}{equation.1.32}{}}
\newlabel{eq:levenberg-marquardt}{{1.33}{17}{Levenberg-Marquardt algorithm}{equation.1.33}{}}
\newlabel{eq:jacobian-entry}{{1.34}{18}{Levenberg-Marquardt algorithm}{equation.1.34}{}}
\newlabel{eq:jacobian-entry-equation}{{1.35}{18}{Levenberg-Marquardt algorithm}{equation.1.35}{}}
\newlabel{eq:levenberg-marquardt-delta}{{1.36}{18}{Levenberg-Marquardt algorithm}{equation.1.36}{}}
\newlabel{eq:jacobian-first-case}{{1.37}{18}{Levenberg-Marquardt algorithm}{equation.1.37}{}}
\newlabel{eq:jacobian-second-case}{{1.38}{18}{Levenberg-Marquardt algorithm}{equation.1.38}{}}
\citation{Mar63}
\citation{Hag94}
\newlabel{eq:jacobian-third-case}{{1.39}{19}{Levenberg-Marquardt algorithm}{equation.1.39}{}}
\newlabel{eq:levenberg-marquardt-inner-neuron}{{1.40a}{19}{Levenberg-Marquardt algorithm}{equation.1.40}{}}
\newlabel{eq:levenberg-marquardt-output-neuron}{{1.40b}{19}{Levenberg-Marquardt algorithm}{equation.1.40}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.3}{\ignorespaces An iteration of the Levenberg-Marquardt training algorithm.\relax }}{19}{algorithm.1.3}}
\newlabel{alg:levenberg-marquart}{{1.3}{19}{An iteration of the Levenberg-Marquardt training algorithm.\relax }{algorithm.1.3}{}}
\citation{Kri}
\citation{Mat16}
\citation{Koh95}
\citation{Mat16}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Practical considerations on the design of artificial neural networks}{20}{subsection.1.2.4}}
\newlabel{section:Practical considerations on the design of artificial neural networks}{{1.2.4}{20}{Practical considerations on the design of artificial neural networks}{subsection.1.2.4}{}}
\citation{OBS}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.4}{\ignorespaces The complete training algorithm adopted in our numerical tests.\relax }}{22}{algorithm.1.4}}
\newlabel{alg:offline-learning-complete}{{1.4}{22}{The complete training algorithm adopted in our numerical tests.\relax }{algorithm.1.4}{}}
\citation{MN16}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reduced basis methods for nonlinear partial differential equations}{23}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Reduced Basis method for nonlinear partial differential equations}{{2}{23}{Reduced basis methods for nonlinear partial differential equations}{chapter.2}{}}
\newlabel{eq:geometric-map}{{2}{23}{Reduced basis methods for nonlinear partial differential equations}{chapter.2}{}}
\citation{HSR16}
\citation{JIR14}
\citation{QMN15}
\newlabel{eq:map-continuous}{{2.1}{24}{Reduced basis methods for nonlinear partial differential equations}{equation.2.1}{}}
\newlabel{eq:nonlinear-system-full}{{2.2}{24}{Reduced basis methods for nonlinear partial differential equations}{equation.2.2}{}}
\newlabel{eq:reduced-solution-algebraic}{{2.3}{24}{Reduced basis methods for nonlinear partial differential equations}{equation.2.3}{}}
\newlabel{eq:reduced-solution}{{2.4}{24}{Reduced basis methods for nonlinear partial differential equations}{equation.2.4}{}}
\citation{MN16}
\citation{Pru02}
\citation{HSR16}
\citation{QMN15}
\citation{MN16}
\citation{Bar04}
\citation{NMA15}
\citation{HSR16}
\citation{QMN15}
\citation{Bal14}
\newlabel{eq:nonlinear-system-reduced}{{2.5}{25}{Reduced basis methods for nonlinear partial differential equations}{equation.2.5}{}}
\citation{Ams10}
\citation{Chen17}
\citation{Haa13}
\newlabel{eq:interpolation-function}{{2.8}{26}{Reduced basis methods for nonlinear partial differential equations}{equation.2.8}{}}
\citation{Qua10}
\citation{MM10}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Parametrized nonlinear PDEs}{27}{section.2.1}}
\newlabel{section:Parametrized nonlinear PDEs}{{2.1}{27}{Parametrized nonlinear PDEs}{section.2.1}{}}
\newlabel{eq:pde-differential-form}{{2.10}{27}{Parametrized nonlinear PDEs}{equation.2.10}{}}
\newlabel{eq:pde-variational-form}{{2.11}{27}{Parametrized nonlinear PDEs}{equation.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Nonlinear Poisson equation}{27}{subsection.2.1.1}}
\newlabel{section:Nonlinear Poisson equation}{{2.1.1}{27}{Nonlinear Poisson equation}{subsection.2.1.1}{}}
\newlabel{eq:poisson-differential}{{2.12}{27}{Nonlinear Poisson equation}{equation.2.12}{}}
\newlabel{eq:poisson-differential-first-equation}{{2.12a}{27}{Nonlinear Poisson equation}{equation.2.12a}{}}
\citation{Ran99}
\newlabel{eq:poisson-weak-derivation}{{2.13}{28}{Nonlinear Poisson equation}{equation.2.13}{}}
\newlabel{eq:poisson-weak-forms}{{2.14}{28}{Nonlinear Poisson equation}{equation.2.14}{}}
\newlabel{eq:poisson-weak}{{2.15}{28}{Nonlinear Poisson equation}{equation.2.15}{}}
\citation{QMN15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Steady Navier-Stokes equations}{29}{subsection.2.1.2}}
\newlabel{section:Steady Navier-Stokes equations}{{2.1.2}{29}{Steady Navier-Stokes equations}{subsection.2.1.2}{}}
\newlabel{eq:ns-differential}{{2.16}{29}{Steady Navier-Stokes equations}{equation.2.16}{}}
\newlabel{eq:mass-conservation}{{2.16a}{29}{Steady Navier-Stokes equations}{equation.2.16a}{}}
\newlabel{eq:momentum-conservation}{{2.16b}{29}{Steady Navier-Stokes equations}{equation.2.16b}{}}
\newlabel{eq:ns-weak}{{2.17}{29}{Steady Navier-Stokes equations}{equation.2.17}{}}
\newlabel{eq:ns-weak-velocity}{{2.17a}{29}{Steady Navier-Stokes equations}{equation.2.17a}{}}
\newlabel{eq:ns-weak-pressure}{{2.17b}{29}{Steady Navier-Stokes equations}{equation.2.17b}{}}
\newlabel{eq:ns-weak-forms}{{2.18}{29}{Steady Navier-Stokes equations}{equation.2.18}{}}
\newlabel{eq:ns-weak-forms-c}{{2.18a}{29}{Steady Navier-Stokes equations}{equation.2.18a}{}}
\newlabel{eq:ns-weak-forms-a}{{2.18b}{29}{Steady Navier-Stokes equations}{equation.2.18b}{}}
\newlabel{eq:ns-weak-forms-b}{{2.18c}{30}{Steady Navier-Stokes equations}{equation.2.18c}{}}
\newlabel{eq:ns-weak-forms-d}{{2.18d}{30}{Steady Navier-Stokes equations}{equation.2.18d}{}}
\newlabel{eq:ns-weak-forms-f1}{{2.18e}{30}{Steady Navier-Stokes equations}{equation.2.18e}{}}
\newlabel{eq:ns-weak-forms-f2}{{2.18f}{30}{Steady Navier-Stokes equations}{equation.2.18f}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}From the original to the reference domain}{30}{section.2.2}}
\newlabel{section:From the original to the reference domain}{{2.2}{30}{From the original to the reference domain}{section.2.2}{}}
\citation{JIR14}
\newlabel{first-compatibility-condition}{{{{(a)}}}{31}{From the original to the reference domain}{Item.12}{}}
\newlabel{second-compatibility-condition}{{{{(b)}}}{31}{From the original to the reference domain}{Item.13}{}}
\newlabel{eq:parametrized-map}{{2.19}{31}{From the original to the reference domain}{equation.2.19}{}}
\newlabel{eq:pde-differential-reference}{{2.20}{31}{From the original to the reference domain}{equation.2.20}{}}
\newlabel{eq:pde-weak-reference}{{2.21}{31}{From the original to the reference domain}{equation.2.21}{}}
\newlabel{eq:parametrized-map-discrete}{{2.23}{31}{From the original to the reference domain}{equation.2.23}{}}
\citation{Rud64}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Change of variables formulae}{32}{subsection.2.2.1}}
\newlabel{section:Change of variables formulae}{{2.2.1}{32}{Change of variables formulae}{subsection.2.2.1}{}}
\newlabel{eq:chain-rule-component}{{2.24}{32}{Change of variables formulae}{equation.2.24}{}}
\newlabel{eq:chain-rule}{{2.25}{32}{Change of variables formulae}{equation.2.25}{}}
\citation{Rud64}
\newlabel{eq:chain-rule-bis}{{2.26}{33}{Change of variables formulae}{equation.2.26}{}}
\newlabel{eq:change-of-variables}{{2.27}{33}{Change of variables formulae}{equation.2.27}{}}
\newlabel{eq:change-of-variables-vectorial}{{2.28}{33}{Change of variables formulae}{equation.2.28}{}}
\newlabel{eq:change-of-variables-first}{{2.29}{33}{Change of variables formulae}{equation.2.29}{}}
\newlabel{eq:change-of-variables-second}{{2.30}{33}{Change of variables formulae}{equation.2.30}{}}
\newlabel{eq:change-of-variables-third}{{2.31}{33}{Change of variables formulae}{equation.2.31}{}}
\newlabel{eq:change-of-variables-fourth}{{2.32}{33}{Change of variables formulae}{equation.2.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The problems of interest}{34}{subsection.2.2.2}}
\newlabel{section:The problems of interest}{{2.2.2}{34}{The problems of interest}{subsection.2.2.2}{}}
\newlabel{eq:poisson-weak-reference}{{2.33}{34}{The problems of interest}{equation.2.33}{}}
\newlabel{eq:poisson-weak-forms-reference}{{2.34}{34}{The problems of interest}{equation.2.34}{}}
\newlabel{eq:poisson-weak-forms-reference-first}{{2.34a}{34}{The problems of interest}{equation.2.34a}{}}
\newlabel{eq:poisson-weak-forms-reference-second}{{2.34b}{34}{The problems of interest}{equation.2.34b}{}}
\newlabel{eq:ns-weak-reference}{{2.35}{34}{The problems of interest}{equation.2.35}{}}
\newlabel{eq:ns-weak-forms}{{2.36}{34}{The problems of interest}{equation.2.36}{}}
\newlabel{eq:ns-weak-forms-c-reference}{{2.36a}{34}{The problems of interest}{equation.2.36a}{}}
\newlabel{eq:ns-weak-forms-a-reference}{{2.36b}{34}{The problems of interest}{equation.2.36b}{}}
\newlabel{eq:ns-weak-forms-b-reference}{{2.36c}{34}{The problems of interest}{equation.2.36c}{}}
\newlabel{eq:ns-weak-forms-d-reference}{{2.36d}{34}{The problems of interest}{equation.2.36d}{}}
\citation{JIR14}
\citation{JIR14}
\newlabel{eq:ns-weak-forms-f1-reference}{{2.36e}{35}{The problems of interest}{equation.2.36e}{}}
\newlabel{eq:ns-weak-forms-f2-reference}{{2.36f}{35}{The problems of interest}{equation.2.36f}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}The boundary displacement-dependent transfinite map (BDD TM)}{35}{subsection.2.2.3}}
\newlabel{section:The boundary displacement-dependent transfinite map}{{2.2.3}{35}{The boundary displacement-dependent transfinite map (BDD TM)}{subsection.2.2.3}{}}
\citation{JIR14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Representation of the boundary conditions for the Laplace problems \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:laplace-problem-weight-function}\unskip \@@italiccorr )}} (left) and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:laplace-problem-projection-function}\unskip \@@italiccorr )}} (right) stated on a exagonal reference domain $\Omega $.\relax }}{36}{figure.caption.15}}
\newlabel{fig:laplace-bc}{{2.1}{36}{Representation of the boundary conditions for the Laplace problems \eqref {eq:laplace-problem-weight-function} (left) and \eqref {eq:laplace-problem-projection-function} (right) stated on a exagonal reference domain $\Omega $.\relax }{figure.caption.15}{}}
\newlabel{eq:laplace-problem-weight-function}{{2.38}{36}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.38}{}}
\newlabel{eq:laplace-problem-projection-function}{{2.39}{36}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.39}{}}
\newlabel{eq:weight-projection-function}{{2.40}{36}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.40}{}}
\citation{Rud64}
\citation{JIR14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }}{37}{figure.caption.16}}
\newlabel{fig:bc-square}{{2.2}{37}{Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }{figure.caption.16}{}}
\newlabel{eq:bddtm}{{2.41}{37}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.41}{}}
\newlabel{eq:my-bddtm}{{2.42}{37}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Well-posedness of the test cases}{37}{section.2.3}}
\newlabel{section:Well-posedness of the test cases}{{2.3}{37}{Well-posedness of the test cases}{section.2.3}{}}
\citation{CR97}
\citation{QMN15}
\citation{ESW04}
\citation{Qua10}
\citation{Rud64}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Finite element method}{38}{section.2.4}}
\newlabel{section:Finite element method}{{2.4}{38}{Finite element method}{section.2.4}{}}
\newlabel{eq:galerkin}{{2.43}{38}{Finite element method}{equation.2.43}{}}
\newlabel{eq:continuity}{{2.44}{39}{Finite element method}{equation.2.44}{}}
\newlabel{eq:inf-sup}{{2.45}{39}{Finite element method}{equation.2.45}{}}
\newlabel{eq:newton-linearized-problem}{{2.46}{39}{Finite element method}{equation.2.46}{}}
\newlabel{eq:galerkin-solution}{{2.47}{39}{Finite element method}{equation.2.47}{}}
\newlabel{eq:galerkin-algebraic}{{2.48}{39}{Finite element method}{equation.2.48}{}}
\newlabel{eq:galerkin-nonlinear-system}{{2.49}{39}{Finite element method}{equation.2.49}{}}
\newlabel{eq:galerkin-nonlinear-system-equation}{{2.50}{39}{Finite element method}{equation.2.50}{}}
\newlabel{eq:galerkin-linear-system}{{2.51}{39}{Finite element method}{equation.2.51}{}}
\citation{QMN15}
\newlabel{eq:notation-1}{{2.53}{40}{Finite element method}{equation.2.53}{}}
\newlabel{eq:notation-2}{{2.54}{40}{Finite element method}{equation.2.54}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces The Newton's method applied to the nonlinear system \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:galerkin-nonlinear-system}\unskip \@@italiccorr )}}.\relax }}{41}{algorithm.2.1}}
\newlabel{alg:newton-full}{{2.1}{41}{The Newton's method applied to the nonlinear system \eqref {eq:galerkin-nonlinear-system}.\relax }{algorithm.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Nonlinear Poisson equation}{41}{subsection.2.4.1}}
\newlabel{section:Nonlinear Poisson equation (FE)}{{2.4.1}{41}{Nonlinear Poisson equation}{subsection.2.4.1}{}}
\newlabel{eq:poisson-residual-vector}{{2.4.1}{41}{Nonlinear Poisson equation}{subsection.2.4.1}{}}
\newlabel{eq:poisson-jacobian}{{2.4.1}{41}{Nonlinear Poisson equation}{subsection.2.4.1}{}}
\citation{Ran99}
\citation{Qua10}
\citation{Per02}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Steady Navier-Stokes equations}{42}{subsection.2.4.2}}
\newlabel{section:Steady Navier-Stokes equations (FE)}{{2.4.2}{42}{Steady Navier-Stokes equations}{subsection.2.4.2}{}}
\newlabel{eq:inf-sup-ns}{{2.55}{42}{Steady Navier-Stokes equations}{equation.2.55}{}}
\newlabel{eq:B}{{2.57c}{43}{Steady Navier-Stokes equations}{equation.2.57c}{}}
\newlabel{eq:ns-system}{{2.58}{43}{Steady Navier-Stokes equations}{equation.2.58}{}}
\citation{Dep08}
\citation{HSR16}
\citation{QMN15}
\citation{HSR16}
\citation{Chen17}
\citation{Vol08}
\newlabel{eq:ns-jacobian}{{2.59}{44}{Steady Navier-Stokes equations}{equation.2.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}POD-Galerkin reduced basis method}{44}{section.2.5}}
\newlabel{section:POD-Galerkin reduced basis method}{{2.5}{44}{POD-Galerkin reduced basis method}{section.2.5}{}}
\citation{HSR16}
\citation{Mad06}
\citation{Mad06}
\citation{Buf12}
\citation{Buf12}
\citation{HSR16}
\citation{Mad06}
\citation{Dep08}
\citation{QMN15}
\newlabel{eq:rb-solution}{{2.60}{45}{POD-Galerkin reduced basis method}{equation.2.60}{}}
\newlabel{eq:kolmogorov-L-width}{{2.62}{45}{}{equation.2.62}{}}
\newlabel{eq:pde-rb}{{2.63}{46}{POD-Galerkin reduced basis method}{equation.2.63}{}}
\newlabel{eq:pde-rb-newton}{{2.64}{46}{POD-Galerkin reduced basis method}{equation.2.64}{}}
\newlabel{eq:rb-fe-coefficients}{{2.65}{46}{POD-Galerkin reduced basis method}{equation.2.65}{}}
\citation{QMN15}
\citation{Vol08}
\citation{Lia02}
\newlabel{eq:rb-algebraic-formulation-1}{{2.66}{47}{POD-Galerkin reduced basis method}{equation.2.66}{}}
\newlabel{eq:rb-nonlinear-system}{{2.67}{47}{POD-Galerkin reduced basis method}{equation.2.67}{}}
\newlabel{eq:rb-nonlinear-system-jacobian}{{2.68}{47}{POD-Galerkin reduced basis method}{equation.2.68}{}}
\newlabel{eq:rb-nonlinear-system-newton}{{2.69}{47}{POD-Galerkin reduced basis method}{equation.2.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Proper Orthogonal Decomposition}{47}{subsection.2.5.1}}
\newlabel{section:Proper Orthogonal Decomposition}{{2.5.1}{47}{Proper Orthogonal Decomposition}{subsection.2.5.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.2}{\ignorespaces The Newton's method applied to the reduced nonlinear system \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:rb-nonlinear-system}\unskip \@@italiccorr )}}.\relax }}{48}{algorithm.2.2}}
\newlabel{alg:newton-rb}{{2.2}{48}{The Newton's method applied to the reduced nonlinear system \eqref {eq:rb-nonlinear-system}.\relax }{algorithm.2.2}{}}
\newlabel{eq:svd}{{2.70}{48}{Proper Orthogonal Decomposition}{equation.2.70}{}}
\citation{Vol08}
\newlabel{eq:left-singular-vectors}{{2.71}{49}{Proper Orthogonal Decomposition}{equation.2.71}{}}
\newlabel{eq:right-singular-vectors}{{2.72}{49}{Proper Orthogonal Decomposition}{equation.2.72}{}}
\newlabel{eq:snapshot-method}{{2.73}{49}{Proper Orthogonal Decomposition}{equation.2.73}{}}
\newlabel{eq:svd-compact}{{2.75}{49}{Proper Orthogonal Decomposition}{equation.2.75}{}}
\newlabel{eq:svd-compact-matrices}{{2.76}{49}{Proper Orthogonal Decomposition}{equation.2.76}{}}
\newlabel{eq:basis-error}{{2.77}{49}{Proper Orthogonal Decomposition}{equation.2.77}{}}
\citation{Vol08}
\citation{Vol08}
\newlabel{eq:pod-optimality}{{2.78}{50}{Schmidt-Eckart-Young}{equation.2.78}{}}
\citation{HSR16}
\citation{Bur06}
\citation{QMN15}
\newlabel{eq:pod-error}{{2.79}{51}{Proper Orthogonal Decomposition}{equation.2.79}{}}
\newlabel{eq:pod-fe-basis-functions}{{2.81}{51}{Proper Orthogonal Decomposition}{equation.2.81}{}}
\newlabel{eq:discrete-scalar-product}{{2.82}{51}{Proper Orthogonal Decomposition}{equation.2.82}{}}
\citation{Pru02}
\newlabel{eq:relative-information-content}{{2.83}{52}{Proper Orthogonal Decomposition}{equation.2.83}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.3}{\ignorespaces The POD algorithm.\relax }}{52}{algorithm.2.3}}
\newlabel{alg:pod}{{2.3}{52}{The POD algorithm.\relax }{algorithm.2.3}{}}
\citation{Bar04}
\citation{MN16}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Implementation: details and issues}{53}{subsection.2.5.2}}
\newlabel{section:Implementation}{{2.5.2}{53}{Implementation: details and issues}{subsection.2.5.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.4}{\ignorespaces The offline and online stages for the POD-Galerkin (POD-G) RB method.\relax }}{53}{algorithm.2.4}}
\newlabel{alg:pod-galerkin}{{2.4}{53}{The offline and online stages for the POD-Galerkin (POD-G) RB method.\relax }{algorithm.2.4}{}}
\citation{QMN15}
\citation{Bal14}
\citation{Bur06}
\citation{Chen17}
\citation{QMN15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Application to the steady Navier-Stokes equations}{54}{subsection.2.5.3}}
\newlabel{section:Application to the steady Navier-Stokes equations}{{2.5.3}{54}{Application to the steady Navier-Stokes equations}{subsection.2.5.3}{}}
\citation{Bur06}
\citation{Bal14}
\newlabel{eq:ns-reduced-system}{{2.84}{55}{Application to the steady Navier-Stokes equations}{equation.2.84}{}}
\newlabel{eq:supremizer-system}{{2.85}{55}{Application to the steady Navier-Stokes equations}{equation.2.85}{}}
\citation{Bal14}
\citation{Bar04}
\newlabel{eq:inf-sup-ns-reduced}{{2.87}{56}{}{equation.2.87}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}A POD-based RB method using neural networks}{56}{section.2.6}}
\newlabel{section:A POD-based RB method using neural networks}{{2.6}{56}{A POD-based RB method using neural networks}{section.2.6}{}}
\citation{Imam08}
\newlabel{eq:linear-poisson}{{2.88}{57}{A POD-based RB method using neural networks}{equation.2.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Left: computational domain $\mathaccentV {widetilde}365{\Omega } = \mathaccentV {widetilde}365{\Omega }(\bm  {\mu })$ (solid line) for the linear Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:linear-poisson}\unskip \@@italiccorr )}}. Right: average relative error committed by approximating the FE solution either through its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal  {P}$ consisting of $N_{te} = 200$ randomly picked values.\relax }}{58}{figure.caption.17}}
\newlabel{fig:linear-poisson}{{2.3}{58}{Left: computational domain $\wt {\Omega } = \wt {\Omega }(\bg {\mu })$ (solid line) for the linear Poisson problem \eqref {eq:linear-poisson}. Right: average relative error committed by approximating the FE solution either through its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal {P}$ consisting of $N_{te} = 200$ randomly picked values.\relax }{figure.caption.17}{}}
\newlabel{eq:nonlinear-poisson}{{2.89}{58}{A POD-based RB method using neural networks}{equation.2.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: computational domain $\mathaccentV {widetilde}365{\Omega } = \mathaccentV {widetilde}365{\Omega }(\bm  {\mu })$ (solid line) for the nonlinear Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:linear-poisson}\unskip \@@italiccorr )}}, with $H = 2$. Right: average relative error committed by approximating the FE solution either through its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue). The errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal  {P}$ consisting of $N_{te} = 200$ randomly picked values.\relax }}{59}{figure.caption.18}}
\newlabel{fig:nonlinear-poisson}{{2.4}{59}{Left: computational domain $\wt {\Omega } = \wt {\Omega }(\bg {\mu })$ (solid line) for the nonlinear Poisson problem \eqref {eq:linear-poisson}, with $H = 2$. Right: average relative error committed by approximating the FE solution either through its projection onto the reduced space (red) or the POD-Galerkin RB solution (blue). The errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal {P}$ consisting of $N_{te} = 200$ randomly picked values.\relax }{figure.caption.18}{}}
\newlabel{eq:high-fidelity-projected}{{2.90}{59}{A POD-based RB method using neural networks}{equation.2.90}{}}
\newlabel{eq:map-to-approximate}{{2.91}{59}{A POD-based RB method using neural networks}{equation.2.91}{}}
\citation{Ams10}
\citation{Chen17}
\newlabel{eq:pod-nn-mse}{{2.94}{61}{A POD-based RB method using neural networks}{equation.2.94}{}}
\newlabel{eq:pod-nn-solution}{{2.95}{61}{A POD-based RB method using neural networks}{equation.2.95}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.5}{\ignorespaces The offline and online stages for the POD-NN RB method.\relax }}{61}{algorithm.2.5}}
\newlabel{alg:pod-nn}{{2.5}{61}{The offline and online stages for the POD-NN RB method.\relax }{algorithm.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}An a priori error analysis}{62}{subsection.2.6.1}}
\newlabel{section:An a priori error analysis}{{2.6.1}{62}{An a priori error analysis}{subsection.2.6.1}{}}
\newlabel{eq:error-analysis-1}{{2.96}{62}{An a priori error analysis}{equation.2.96}{}}
\newlabel{eq:error-analysis-2}{{2.97}{62}{An a priori error analysis}{equation.2.97}{}}
\newlabel{eq:error-analysis-3}{{2.99}{63}{An a priori error analysis}{equation.2.99}{}}
\newlabel{eq:error-analysis-4}{{2.100}{63}{An a priori error analysis}{equation.2.100}{}}
\newlabel{eq:error-analysis-5}{{2.101}{63}{An a priori error analysis}{equation.2.101}{}}
\newlabel{eq:error-analysis-final}{{2.102}{64}{An a priori error analysis}{equation.2.102}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.6}{\ignorespaces Selection of an optimal network configuration.\relax }}{64}{algorithm.2.6}}
\newlabel{alg:podnn-training}{{2.6}{64}{Selection of an optimal network configuration.\relax }{algorithm.2.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Numerical results}{65}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Numerical results}{{3}{65}{Numerical results}{chapter.3}{}}
\bibcite{Ams10}{1}
\bibcite{Bal14}{2}
\bibcite{Bar04}{3}
\bibcite{Buf12}{4}
\bibcite{Bur06}{5}
\bibcite{CR97}{6}
\bibcite{Chen17}{7}
\bibcite{Cyb88}{8}
\bibcite{Cyb89}{9}
\bibcite{Dep08}{10}
\bibcite{ESW04}{11}
\bibcite{Fah88}{12}
\bibcite{Hag94}{13}
\bibcite{Hag14}{14}
\bibcite{Haa13}{15}
\bibcite{Hay05}{16}
\bibcite{Heb49}{17}
\bibcite{Lia02}{18}
\bibcite{OBS}{19}
\bibcite{HSR16}{20}
\bibcite{Hop82}{21}
\bibcite{Imam08}{22}
\bibcite{JIR14}{23}
\bibcite{KLM96}{24}
\bibcite{Koh95}{25}
\bibcite{Koh98}{26}
\bibcite{Kri}{27}
\bibcite{Mad06}{28}
\bibcite{Mar63}{29}
\bibcite{Mat16}{30}
\bibcite{MM10}{31}
\bibcite{MN16}{32}
\bibcite{MR86}{33}
\bibcite{Nie15}{34}
\bibcite{NMA15}{35}
\bibcite{Per02}{36}
\bibcite{Pru02}{37}
\bibcite{Qua10}{38}
\bibcite{QMN15}{39}
\bibcite{Ran99}{40}
\bibcite{RB93}{41}
\bibcite{Ros58}{42}
\bibcite{Rud64}{43}
\bibcite{SD13}{44}
\bibcite{Vol08}{45}
\bibcite{WH60}{46}
